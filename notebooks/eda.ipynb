{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import copy\n",
    "from itertools import cycle\n",
    "from typing import Optional, Union\n",
    "\n",
    "import os, gc\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import requests_cache\n",
    "import numpy as np\n",
    "from sympy.physics.units import years\n",
    "\n",
    "from data_collection_modules import DataEnergySMARD\n",
    "from data_collection_modules import locations, OpenMeteo"
   ],
   "id": "277503e23704893c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load All Dataframes",
   "id": "9925f0c4c3b6130a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "data_dir = '../database/'\n",
    "target = 'wind_onshore'#'wind_offshore'\n",
    "# df = pd.read_parquet(data_dir + 'latest.parquet')\n",
    "df_smard = pd.read_parquet(data_dir + 'smard/' + 'history.parquet')\n",
    "df_om = pd.read_parquet(data_dir + 'openmeteo/' + 'history.parquet')\n",
    "df_om_f = pd.read_parquet(data_dir + 'openmeteo/' + 'forecast.parquet')\n",
    "df_es = pd.read_parquet(data_dir + 'epexspot/' + 'history.parquet')\n",
    "df_entsoe = pd.read_parquet(data_dir + 'entsoe/' + 'history.parquet')\n",
    "\n",
    "print(f\"SMARD data shapes hist={df_smard.shape} (days={len(df_smard)/24}) start={df_smard.index[0]} end={df_smard.index[-1]}\")\n",
    "print(f\"ENTSOE data shapes hist={df_entsoe.shape} (days={len(df_entsoe)/24}) start={df_entsoe.index[0]} end={df_entsoe.index[-1]}\")\n",
    "print(f\"Openmeteo data shapes hist={df_om.shape} (days={len(df_om)/24}) start={df_om.index[0]} end={df_om.index[-1]}\")\n",
    "print(f\"Openmeteo data shapes forecast={df_om_f.shape} (days={len(df_om_f)/24}) start={df_om_f.index[0]} end={df_om_f.index[-1]}\")\n",
    "print(f\"EPEXSPOT data shapes hist={df_es.shape} (days={len(df_es)/24}) start={df_es.index[0]} end={df_es.index[-1]}\")\n",
    "\n",
    "print(f\"Target={target} Nans={df_smard[target].isna().sum().sum()}\")\n",
    "# set how to split the dataset\n",
    "cutoff = df_om_f.index[0]\n",
    "if cutoff == cutoff.normalize():\n",
    "    print(f\"The cutoff timestamp corresponds to the beginning of the day {cutoff.normalize()}\")\n",
    "print(f\"Dataset is split into ({len(df_om[:cutoff])}) before and \"\n",
    "      f\"({len(df_om_f[cutoff:])}) ({int(len(df_om_f[cutoff:])/24)} days) after {cutoff}.\")\n",
    "# print(df_smard.columns.to_list())\n",
    "# print(df_om.columns.to_list())\n",
    "print(df_entsoe.columns.to_list())\n",
    "# print(locations)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# idx = df_om.index[-1]-timedelta(days=14) # separate historic and forecasted data\n",
    "# df_om_h = df_om[:idx]\n",
    "# df_om_f = df_om[idx+timedelta(hours=1):]\n",
    "# print(f\"Openmeteo data shapes hist={df_om_h.shape} (days={len(df_om_h)/24}) start={df_om_h.index[0]} end={df_om_h.index[-1]}\")\n",
    "# print(f\"Openmeteo data shapes forecast={df_om_f.shape} (days={len(df_om_f)/24}) start={df_om_f.index[0]} end={df_om_f.index[-1]}\")\n",
    "# df_om_h.to_parquet(data_dir + 'openmeteo/' + 'history.parquet')\n",
    "# df_om_f.to_parquet(data_dir + 'openmeteo/' + 'forecast.parquet')"
   ],
   "id": "e1100f7929fde501",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# df_smard = pd.read_parquet(data_dir + 'smard/' + 'history.parquet')\n",
    "# df_smard_ = df_smard[:df_smard.index[-1]-timedelta(days=180)]\n",
    "# df_smard_.to_parquet(data_dir + 'smard/' + 'history.parquet')\n",
    "fig, ax = plt.subplots()\n",
    "# df_smard['wind_onshore'].tail(15*24).plot(ax=ax)\n",
    "# df_smard['wind_offshore'].tail(15*24).plot(ax=ax)\n",
    "# df_om['wind_speed_10m_woff_enbw'].tail(7*24).plot(ax=ax)\n",
    "df_om['wind_gusts_10m_woff_enbw'].plot(ax=ax)\n",
    "# df_om_f['wind_speed_10m_woff_enbw'].head(7*24).plot(ax=ax, ls='--')\n",
    "df_om_f['wind_gusts_10m_woff_enbw'].plot(ax=ax, ls='--')"
   ],
   "id": "a209f4ed67afabb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Engineering For Weather Data (Historic and Forecast)",
   "id": "fc80c858812d488b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# df_entsoe.drop(['Solar Actual Consumption_tran', 'Wind Onshore Actual Consumption_tran'],inplace=True, axis=1)\n",
    "# df_entsoe.to_parquet(data_dir + 'entsoe/' + 'history.parquet')\n",
    "# df_entsoe.columns.to_list()"
   ],
   "id": "64673542502db37c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_collection_modules.locations import de_regions\n",
    "df_entsoe['wind_onshore'] = sum([df_entsoe[f\"wind_onshore{reg['suffix']}\"] for reg in de_regions if f\"wind_onshore{reg['suffix']}\" in df_entsoe])\n",
    "df_entsoe['wind_offshore'] = sum([df_entsoe[f\"wind_offshore{reg['suffix']}\"] for reg in de_regions if f\"wind_offshore{reg['suffix']}\" in df_entsoe])\n",
    "df_entsoe['solar'] = sum([df_entsoe[f\"solar{reg['suffix']}\"] for reg in de_regions])\n",
    "fig,axes = plt.subplots(ncols=1,nrows=3,sharex='all')\n",
    "tail=30*24\n",
    "for i, qant in enumerate(['wind_onshore','wind_offshore','solar']):\n",
    "    ax = axes[i]\n",
    "    ax.plot(df_entsoe.tail(tail).index, df_entsoe.tail(tail)[qant], color='blue',label='ENTSOE',ls='-', lw=0.6)\n",
    "    ax.plot(df_smard.tail(tail).index, df_smard.tail(tail)[qant], color='red',label='SMARD',ls='--', lw=0.6)\n",
    "plt.legend()"
   ],
   "id": "e9e3ae4acde46775",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create combined dataframe111\n",
    "cutoff = df_om_f.index[0]\n",
    "df_om = df_om.combine_first(df_om_f)\n",
    "if df_om.isna().any().any():\n",
    "    print(\"ERROR! Nans in the dataframe\")\n",
    "# df_om[df_om.index[-1]-pd.Timedelta(hours = 101 * 7 * 24 - 1):].to_parquet(\"../tmp_database/\" + 'openmeteo.parquet')\n",
    "# pd.DataFrame(df_smard[df_smard.index[-1]-pd.Timedelta(hours = 101 * 7 * 24 - 1):][target]).to_parquet(\"../tmp_database/\" + 'smard_target.parquet')\n",
    "def preprocess_openmeteo_for_offshore_wind_OLD(df, location_suffix=\"_hsee\"):\n",
    "    \"\"\"\n",
    "    Preprocesses weather data for forecasting offshore wind energy generation.\n",
    "    \"\"\"\n",
    "    # 1. Filter for relevant location\n",
    "    cols_to_keep = [c for c in df.columns if c.endswith(location_suffix)]\n",
    "    df = df[cols_to_keep].copy()\n",
    "\n",
    "    # 2. Key variable columns\n",
    "    wind_speed_10m_col = f\"wind_speed_10m{location_suffix}\"\n",
    "    wind_speed_100m_col = f\"wind_speed_100m{location_suffix}\"\n",
    "    wind_dir_10m_col = f\"wind_direction_10m{location_suffix}\"\n",
    "    wind_dir_100m_col = f\"wind_direction_100m{location_suffix}\"\n",
    "    temp_col = f\"temperature_2m{location_suffix}\"\n",
    "    press_col = f\"surface_pressure{location_suffix}\"\n",
    "    rh_col = f\"relative_humidity_2m{location_suffix}\"\n",
    "\n",
    "    # 3. Wind direction (cyclic encoding)\n",
    "    if wind_dir_10m_col in df.columns:\n",
    "        df[\"wind_dir_10m_sin\"] = np.sin(np.deg2rad(df[wind_dir_10m_col]))\n",
    "        df[\"wind_dir_10m_cos\"] = np.cos(np.deg2rad(df[wind_dir_10m_col]))\n",
    "        df.drop(columns=[wind_dir_10m_col], inplace=True, errors=\"ignore\")\n",
    "    if wind_dir_100m_col in df.columns:\n",
    "        df[\"wind_dir_100m_sin\"] = np.sin(np.deg2rad(df[wind_dir_100m_col]))\n",
    "        df[\"wind_dir_100m_cos\"] = np.cos(np.deg2rad(df[wind_dir_100m_col]))\n",
    "        df.drop(columns=[wind_dir_100m_col], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # 4. Wind speed transformations\n",
    "    if wind_speed_10m_col in df.columns and wind_speed_100m_col in df.columns:\n",
    "        # Wind Shear\n",
    "        df[\"wind_shear\"] = np.log(df[wind_speed_100m_col] / df[wind_speed_10m_col]) / np.log(100 / 10)\n",
    "        # Wind energy potential (v^3 for both heights)\n",
    "        df[\"wind_power_potential_10m\"] = df[wind_speed_10m_col] ** 3\n",
    "        df[\"wind_power_potential_100m\"] = df[wind_speed_100m_col] ** 3\n",
    "\n",
    "        # Lag and rolling features for wind speed at 100m\n",
    "        for lag in [1, 6, 12, 24]:\n",
    "            df[f\"{wind_speed_100m_col}_lag{lag}\"] = df[wind_speed_100m_col].shift(lag)\n",
    "        df[f\"{wind_speed_100m_col}_roll6\"] = df[wind_speed_100m_col].rolling(window=6, min_periods=1).mean()\n",
    "        df[f\"{wind_speed_100m_col}_roll24\"] = df[wind_speed_100m_col].rolling(window=24, min_periods=1).mean()\n",
    "\n",
    "    # 5. Turbulence Intensity\n",
    "    if wind_speed_100m_col in df.columns:\n",
    "        rolling_std = df[wind_speed_100m_col].rolling(window=3, min_periods=1).std()\n",
    "        rolling_mean = df[wind_speed_100m_col].rolling(window=3, min_periods=1).mean()\n",
    "        df[\"turbulence_intensity\"] = rolling_std / rolling_mean\n",
    "\n",
    "    # 6. Wind Ramp Events\n",
    "    if wind_speed_100m_col in df.columns:\n",
    "        df[\"wind_ramp\"] = df[wind_speed_100m_col].diff(periods=1)  # Change in wind speed over 1 timestep\n",
    "\n",
    "    # 7. Moisture Index\n",
    "    if temp_col in df.columns and rh_col in df.columns:\n",
    "        df[\"moisture_index\"] = df[temp_col] * df[rh_col]\n",
    "\n",
    "    # 8. Thermal Stability Index\n",
    "    if temp_col in df.columns and press_col in df.columns:\n",
    "        df[\"thermal_stability_index\"] = (df[temp_col] + 273.15) * (1000 / df[press_col]) ** 0.286\n",
    "\n",
    "    # 9. Drop irrelevant features\n",
    "    drop_vars = [\"precipitation\", \"cloud_cover\", \"shortwave_radiation\"]\n",
    "    drop_cols = [f\"{var}{location_suffix}\" for var in drop_vars if f\"{var}{location_suffix}\" in df.columns]\n",
    "    df.drop(columns=drop_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    return df\n",
    "def preprocess_openmeteo_for_offshore_wind(df, location_suffix=\"_hsee\")->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses weather data for forecasting offshore wind energy generation.\n",
    "    Focuses on critical physical features and includes turbulence_intensity, wind_ramp, and wind_shear.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Filter for the offshore wind farm location only\n",
    "    cols_to_keep = [c for c in df.columns if c.endswith(location_suffix)]\n",
    "    df = df[cols_to_keep].copy()\n",
    "\n",
    "    # 2. Define key variable columns\n",
    "    wind_speed_10m_col = f\"wind_speed_10m{location_suffix}\"\n",
    "    wind_speed_100m_col = f\"wind_speed_100m{location_suffix}\"\n",
    "    wind_dir_100m_col = f\"wind_direction_100m{location_suffix}\"\n",
    "    temp_col = f\"temperature_2m{location_suffix}\"\n",
    "    press_col = f\"surface_pressure{location_suffix}\"\n",
    "\n",
    "    # 3. Compute Air Density (ρ)\n",
    "    if temp_col in df.columns and press_col in df.columns:\n",
    "        temp_K = df[temp_col] + 273.15\n",
    "        R_specific = 287.05  # J/(kg·K) for dry air\n",
    "        # convert pressure from hPa to Pa\n",
    "        df[\"air_density\"] = np.array( (df[press_col] * 100.) / (R_specific * temp_K) )\n",
    "\n",
    "    # 4. Compute Wind Power Density (if wind_speed_100m and air_density are available)\n",
    "    if wind_speed_100m_col in df.columns and \"air_density\" in df.columns:\n",
    "        df[\"wind_power_density\"] = np.array( 0.5 * df[\"air_density\"] * (df[wind_speed_100m_col] ** 3) )\n",
    "\n",
    "    # 5. Encode Wind Direction (Cyclic)\n",
    "    if wind_dir_100m_col in df.columns:\n",
    "        df[\"wind_dir_sin\"] = np.sin(np.deg2rad(df[wind_dir_100m_col]))\n",
    "        df[\"wind_dir_cos\"] = np.cos(np.deg2rad(df[wind_dir_100m_col]))\n",
    "        df.drop(columns=[wind_dir_100m_col], inplace=True)\n",
    "\n",
    "    # 6. Wind Shear (Requires both 10m and 100m wind speeds)\n",
    "    if wind_speed_10m_col in df.columns and wind_speed_100m_col in df.columns:\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            df[\"wind_shear\"] = np.log(df[wind_speed_100m_col] / df[wind_speed_10m_col]) / np.log(100/10)\n",
    "        # Replace infinities or NaNs if they occur\n",
    "        df[\"wind_shear\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # 7. Turbulence Intensity (using a short rolling window on 100m wind speed)\n",
    "    if wind_speed_100m_col in df.columns:\n",
    "        rolling_std = df[wind_speed_100m_col].rolling(window=3, min_periods=1).std()\n",
    "        rolling_mean = df[wind_speed_100m_col].rolling(window=3, min_periods=1).mean()\n",
    "        df[\"turbulence_intensity\"] = np.array( rolling_std / rolling_mean )\n",
    "\n",
    "    # 8. Wind Ramp (difference in 100m wind speed over 1 timestep)\n",
    "    if wind_speed_100m_col in df.columns:\n",
    "        df[\"wind_ramp\"] = df[wind_speed_100m_col].diff(1)\n",
    "\n",
    "    # 9. Lag Features for Wind Speed at 100m\n",
    "    if wind_speed_100m_col in df.columns:\n",
    "        for lag in [1, 6, 12, 24]:\n",
    "            df[f\"wind_speed_lag_{lag}\"] = df[wind_speed_100m_col].shift(lag)\n",
    "\n",
    "    # 11. Drop Irrelevant Columns\n",
    "    # Decide which columns to drop. For model simplicity, consider dropping raw weather inputs\n",
    "    # that have been transformed into more physical parameters.\n",
    "    # However, keep wind speeds if you think they add value.\n",
    "    # For now, we keep the wind speeds since other derived features depend on them.\n",
    "    drop_vars = [\n",
    "        temp_col, press_col, \"air_density\",\n",
    "        f\"precipitation{location_suffix}\",\n",
    "        f\"cloud_cover{location_suffix}\",\n",
    "        f\"shortwave_radiation{location_suffix}\",\n",
    "        f\"relative_humidity_2m{location_suffix}\",\n",
    "        f\"wind_direction_10m{location_suffix}\",\n",
    "        f\"wind_gusts_10m{location_suffix}\"\n",
    "    ]\n",
    "    drop_cols = [c for c in drop_vars if c in df.columns]\n",
    "    df.drop(columns=drop_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # Handle missing values introduced by lagging and other computations\n",
    "    # df.dropna(inplace=True)\n",
    "\n",
    "    return df\n",
    "df_om_prep = preprocess_openmeteo_for_offshore_wind(df=df_om, location_suffix=\"_hsee\")\n",
    "print(df_om_prep.shape, df_om_prep.columns, df_om_prep.isna().sum())\n",
    "df_om_prep.dropna(inplace=True)"
   ],
   "id": "ca3d11c66e2f67dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_weather_smard(df_om_prep_:pd.DataFrame, df_smard_:pd.DataFrame, target:str, ylabel:str):\n",
    "    fig, axes = plt.subplots(ncols=1, nrows=6, figsize=(8, 8), sharex='all')\n",
    "    # Plot data\n",
    "    axes[0].plot(df_smard_.index, df_smard_[target], label=target, color='black', lw=0.6)\n",
    "    axes[0].set_ylabel(ylabel)\n",
    "\n",
    "    axes[1].plot(df_om_prep_.index, df_om_prep_[\"wind_speed_100m_hsee\"], label=\"Wind Speed 100m\", color='black', lw=0.6)\n",
    "    axes[1].set_ylabel(\"Wind Speed 100m\")\n",
    "\n",
    "    axes[2].plot(df_om_prep_.index, df_om_prep_[\"wind_shear\"], label=\"Wind Shear\", color='black', lw=0.6)\n",
    "    axes[2].set_ylabel(\"Wind Shear\")\n",
    "\n",
    "    axes[3].plot(df_om_prep_.index, df_om_prep_[\"turbulence_intensity\"], label=\"Turbulence Intensity\", color='black', lw=0.6)\n",
    "    axes[3].set_ylabel(\"Turbulence Intensity\")\n",
    "\n",
    "    axes[4].plot(df_om_prep_.index, df_om_prep_[\"wind_ramp\"], label=\"Wind Ramp\", color='black', lw=0.6)\n",
    "    axes[4].set_ylabel(\"Wind Ramp\")\n",
    "\n",
    "    axes[5].plot(df_om_prep_.index, df_om_prep_[\"wind_power_density\"], label=\"Wind Power Density\", color='black', lw=0.6)\n",
    "    axes[5].set_ylabel(\"Wind Power Density\")\n",
    "\n",
    "    # Configure transparent axes and add gray gridlines\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.grid(True, linestyle='-', alpha=0.4)\n",
    "        ax.tick_params(axis='x', direction='in', bottom=True)\n",
    "        ax.tick_params(axis='y', which='both', direction='in', left=True, right=True)\n",
    "        # Set border lines transparent by setting the edge color and alpha\n",
    "        ax.spines['top'].set_edgecolor((1, 1, 1, 0))  # Transparent top border\n",
    "        ax.spines['right'].set_edgecolor((1, 1, 1, 0))  # Transparent right border\n",
    "        ax.spines['left'].set_edgecolor((1, 1, 1, 0))  # Transparent left border\n",
    "        ax.spines['bottom'].set_edgecolor((1, 1, 1, 0))  # Transparent bottom border\n",
    "\n",
    "        # Make x and y ticks transparent\n",
    "        ax.tick_params(axis='x', color=(1, 1, 1, 0))  # Transparent x ticks\n",
    "        ax.tick_params(axis='y', color=(1, 1, 1, 0))  # Transparent y ticks\n",
    "\n",
    "        if i == len(axes) - 1:\n",
    "            ax.xaxis.set_major_locator(mdates.DayLocator())\n",
    "            # ax_bottom.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))  # Format as \"Dec 15\"\n",
    "        fig.autofmt_xdate(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./engineered_features.png',dpi=600)\n",
    "    plt.show()\n",
    "visualize_weather_smard(df_om_prep[:cutoff].tail(30*24), df_smard[:cutoff].tail(30*24), target=target, ylabel='Offs. Wind Power')"
   ],
   "id": "e879dd3ea2110d8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_correlations(df_om_prep_: pd.DataFrame, df_smard_: pd.DataFrame, target: str, label:str):\n",
    "    # Ensure the target column exists in df_smard_\n",
    "    if target not in df_smard_.columns:\n",
    "        raise ValueError(f\"Target column '{target}' not found in the provided df_smard_ DataFrame.\")\n",
    "\n",
    "    # Extract the target series\n",
    "    target_series = df_smard_[target]\n",
    "\n",
    "    # Compute correlations with predictors in df_om_prep_\n",
    "    correlations = df_om_prep_.apply(lambda col: col.corr(target_series))\n",
    "\n",
    "    # Drop NaN values (e.g., if correlation couldn't be computed)\n",
    "    correlations = correlations.dropna()\n",
    "\n",
    "    # Format feature names for display\n",
    "    formatted_feature_names = [\n",
    "        col.replace('_', ' ').title() for col in correlations.index\n",
    "    ]\n",
    "\n",
    "    # Create a figure\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    # Bar plot for correlations\n",
    "    bars = ax.barh(formatted_feature_names, correlations.values, color='black', alpha=0.8)\n",
    "\n",
    "    # Add gridlines\n",
    "    ax.grid(True, linestyle='-', alpha=0.4, axis='x')\n",
    "\n",
    "    # Set transparent borders\n",
    "    ax.spines['top'].set_edgecolor((1, 1, 1, 0))\n",
    "    ax.spines['right'].set_edgecolor((1, 1, 1, 0))\n",
    "    ax.spines['left'].set_edgecolor((1, 1, 1, 0))\n",
    "    ax.spines['bottom'].set_edgecolor((1, 1, 1, 0))\n",
    "\n",
    "    # Set x and y ticks transparent\n",
    "    ax.tick_params(axis='x', color=(1, 1, 1, 0))\n",
    "    ax.tick_params(axis='y', color=(1, 1, 1, 0))\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_xlabel(\"Correlation\", color='black')\n",
    "    ax.set_ylabel(\"Features\", color='black')\n",
    "    ax.set_title(f\"Correlation with {label}\", color='black')\n",
    "\n",
    "    # Add values on bars\n",
    "    for bar in bars:\n",
    "        ax.text(\n",
    "            bar.get_width(),\n",
    "            bar.get_y() + bar.get_height() / 2,\n",
    "            f'{bar.get_width():.2f}',\n",
    "            va='center',\n",
    "            ha='left',\n",
    "            color='gray',\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./feature_target_correlations.png',dpi=600)\n",
    "    plt.show()\n",
    "plot_correlations(df_om_prep[:cutoff], df_smard[:cutoff], target=target, label='Offshore Wind Power')"
   ],
   "id": "371ca2d41880e1f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from itertools import combinations\n",
    "from forecasting_modules.tasks import TaskPaths\n",
    "def plot_optuna_results(outdir: str):\n",
    "    \"\"\"\n",
    "    Load the saved Optuna results and plot the optimization history, parameter distributions,\n",
    "    and a comprehensive parallel coordinate plot.\n",
    "\n",
    "    Args:\n",
    "        outdir (str): Directory where the Optuna results are saved.\n",
    "    \"\"\"\n",
    "    # Load the complete study results from CSV\n",
    "    results_file = os.path.join(outdir, '_complete_study_results.csv')\n",
    "    if not os.path.isfile(results_file):\n",
    "        raise FileNotFoundError(f\"The file {results_file} does not exist.\")\n",
    "\n",
    "    results_df = pd.read_csv(results_file)\n",
    "\n",
    "    # Plot optimization history\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results_df['number'], results_df['value'], marker='o', linestyle='-', label='Trial Objective Value')\n",
    "    plt.title('Optimization History')\n",
    "    plt.xlabel('Trial Number')\n",
    "    plt.ylabel('Objective Value')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot parameter distributions\n",
    "    param_columns = [col for col in results_df.columns if col.startswith('params_')]\n",
    "\n",
    "    if param_columns:\n",
    "        for param in param_columns:\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            sns.histplot(results_df[param], kde=True, bins=20)\n",
    "            plt.title(f\"Distribution of {param.replace('params_', '')}\")\n",
    "            plt.xlabel(param.replace('params_', ''))\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No parameter data found in the results.\")\n",
    "\n",
    "    # Parallel coordinate plot\n",
    "    if param_columns:\n",
    "        parallel_df = results_df[['value'] + param_columns].copy()\n",
    "        parallel_df = parallel_df.dropna()  # Remove NaN values for plotting\n",
    "        parallel_df['value'] = -parallel_df['value']  # Negate if lower objective is better for visualization\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        parallel_coordinates(parallel_df, class_column='value', colormap='viridis')\n",
    "        plt.title('Parallel Coordinate Plot for Parameters and Objective Value')\n",
    "        plt.xlabel('Parameters and Objective')\n",
    "        plt.ylabel('Values')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No parameter data found for the parallel coordinate plot.\")\n",
    "def plot_optuna_study_grid(outdir: str):\n",
    "    \"\"\"\n",
    "    Load the saved Optuna study results and create a grid of contour plots for the top 4 most important features.\n",
    "\n",
    "    Args:\n",
    "        outdir (str): Directory where the study results were saved.\n",
    "    \"\"\"\n",
    "    # Load complete study results from CSV\n",
    "    complete_results_path = os.path.join(outdir, '_complete_study_results.csv')\n",
    "    if not os.path.isfile(complete_results_path):\n",
    "        raise FileNotFoundError(f\"Complete study results file not found: {complete_results_path}\")\n",
    "\n",
    "    study_results = pd.read_csv(complete_results_path)\n",
    "\n",
    "    # Ensure 'value' column exists for the objective\n",
    "    if 'value' not in study_results.columns:\n",
    "        raise ValueError(\"The study results file does not contain the 'value' column.\")\n",
    "\n",
    "    # Extract parameters and objective values\n",
    "    param_columns = [col for col in study_results.columns if col.startswith('params_')]\n",
    "    if len(param_columns) < 2:\n",
    "        raise ValueError(\"At least two parameters are required to create contour plots.\")\n",
    "\n",
    "    study_results = study_results.dropna(subset=['value'])  # Remove trials with no objective value\n",
    "\n",
    "    # Calculate importance of each parameter based on correlation with the target value\n",
    "    importance_scores = {\n",
    "        param: abs(study_results[[param, 'value']].corr().iloc[0, 1])\n",
    "        for param in param_columns\n",
    "    }\n",
    "\n",
    "    # Select the top 4 most important parameters\n",
    "    top_params = sorted(importance_scores, key=importance_scores.get, reverse=True)[:4]\n",
    "\n",
    "    # Prepare a grid of contour plots for top parameter pairs\n",
    "    num_params = len(top_params)\n",
    "    param_names = [param.replace('params_', '') for param in top_params]\n",
    "\n",
    "    fig, axes = plt.subplots(num_params - 1, num_params - 1, figsize=(15, 15), constrained_layout=True)\n",
    "\n",
    "    for i, param1 in enumerate(top_params[:-1]):\n",
    "        for j, param2 in enumerate(top_params[i + 1:]):\n",
    "            ax = axes[i, j] if num_params > 2 else axes\n",
    "\n",
    "            # Extract data for the current pair of parameters\n",
    "            x = study_results[param1]\n",
    "            y = study_results[param2]\n",
    "            z = study_results['value']\n",
    "\n",
    "            # Create a scatter plot with contour\n",
    "            sc = ax.scatter(x, y, c=z, cmap='viridis', s=20)\n",
    "            plt.colorbar(sc, ax=ax, label='Objective Value')\n",
    "\n",
    "            ax.set_xlabel(param_names[i])\n",
    "            ax.set_ylabel(param_names[j])\n",
    "            ax.set_title(f'{param_names[i]} vs {param_names[j]}')\n",
    "\n",
    "    plt.suptitle('Contour Plots of Objective Value for Top Parameter Pairs', fontsize=16)\n",
    "    plt.show()\n",
    "def load_and_plot_parallel_coordinates_(outdir:str):\n",
    "    \"\"\"\n",
    "    Load the complete study results from a CSV file and create a parallel coordinates plot.\n",
    "\n",
    "    Args:\n",
    "        study_results_csv (str): Path to the CSV file containing the study results.\n",
    "        objective_column (str): The column name for the objective value. Defaults to 'value'.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    objective_column = 'value'\n",
    "\n",
    "    complete_results_path = os.path.join(outdir, '_complete_study_results.csv')\n",
    "    if not os.path.isfile(complete_results_path):\n",
    "        raise FileNotFoundError(f\"Complete study results file not found: {complete_results_path}\")\n",
    "\n",
    "    df = pd.read_csv(complete_results_path)\n",
    "\n",
    "    # Normalize each parameter and the objective value\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(df.drop(columns=['state']))\n",
    "    normalized_df = pd.DataFrame(normalized_data, columns=df.columns.drop('state'))\n",
    "    # normalized_df.drop('number',inplace=True)\n",
    "    normalized_df.drop('number', axis=1, inplace=True)\n",
    "\n",
    "    # Extract parameters and the objective value\n",
    "    parameters = normalized_df.drop(columns=[objective_column])\n",
    "    objective_values = normalized_df[objective_column]\n",
    "\n",
    "    # Convert objective values to grayscale for color coding\n",
    "    grayscale_colors = plt.cm.gray(1 - objective_values.values)\n",
    "\n",
    "    # Create the parallel coordinates plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    for i in range(len(parameters)):\n",
    "        ax.plot(\n",
    "            range(len(parameters.columns)),\n",
    "            parameters.iloc[i],\n",
    "            color=grayscale_colors[i],\n",
    "            linewidth=0.8,\n",
    "        )\n",
    "\n",
    "    # Set x-axis ticks and labels\n",
    "    ax.set_xticks(range(len(parameters.columns)), parameters.columns, rotation=75)\n",
    "    ax.set_xlabel(\"Parameters\")\n",
    "    ax.set_ylabel(\"Normalized Values\")\n",
    "    ax.set_title(\"Parallel Coordinates Plot of Optuna Study\")\n",
    "\n",
    "    # Add a color bar for grayscale objective values\n",
    "    sm = plt.cm.ScalarMappable(cmap=\"gray\", norm=plt.Normalize(vmin=df[objective_column].min(), vmax=df[objective_column].max()))\n",
    "    sm.set_array(objective_values.values)\n",
    "    cbar = plt.colorbar(sm, ax=plt.gca())\n",
    "    cbar.set_label(\"Objective Value\")\n",
    "\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def load_and_plot_parallel_coordinates(outdir: str):\n",
    "    \"\"\"\n",
    "    Load the complete study results from a CSV file and create a parallel coordinates plot.\n",
    "\n",
    "    Args:\n",
    "        outdir (str): Path to the directory containing the study results.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    objective_column = 'value'\n",
    "\n",
    "    # Load the data\n",
    "    complete_results_path = os.path.join(outdir, '_complete_study_results.csv')\n",
    "    if not os.path.isfile(complete_results_path):\n",
    "        raise FileNotFoundError(f\"Complete study results file not found: {complete_results_path}\")\n",
    "\n",
    "    df = pd.read_csv(complete_results_path)\n",
    "\n",
    "    # Normalize each parameter and the objective value\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(df.drop(columns=['state']))\n",
    "    normalized_df = pd.DataFrame(normalized_data, columns=df.columns.drop('state'))\n",
    "    normalized_df.drop('number', axis=1, inplace=True)\n",
    "\n",
    "    # Extract parameters and the objective value\n",
    "    parameters = normalized_df.drop(columns=[objective_column])\n",
    "    objective_values = normalized_df[objective_column]\n",
    "\n",
    "    # Identify the indices of the top 3 runs with the lowest objective values\n",
    "    top_3_indices = df.nsmallest(3, objective_column).index\n",
    "\n",
    "    # Convert objective values to grayscale for color coding\n",
    "    grayscale_colors = plt.cm.gray(1 - objective_values.values)\n",
    "\n",
    "    # Create the parallel coordinates plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    for i in range(len(parameters)):\n",
    "        if i in top_3_indices:\n",
    "            if i == top_3_indices[0]:\n",
    "                color = 'red'\n",
    "            elif i == top_3_indices[1]:\n",
    "                color = 'green'\n",
    "            elif i == top_3_indices[2]:\n",
    "                color = 'blue'\n",
    "            linewidth = 1.5\n",
    "        else:\n",
    "            color = grayscale_colors[i]\n",
    "            linewidth = 0.5\n",
    "\n",
    "        ax.plot(\n",
    "            range(len(parameters.columns)),\n",
    "            parameters.iloc[i],\n",
    "            color=color,\n",
    "            linewidth=linewidth,\n",
    "        )\n",
    "\n",
    "    # Set x-axis ticks and labels\n",
    "    ax.set_xticks(range(len(parameters.columns)), parameters.columns, rotation=75)\n",
    "    # ax.set_xlabel(\"Parameters\")\n",
    "    # ax.set_ylabel(\"Normalized Values\")\n",
    "    ax.set_title(\"Parallel Coordinates Plot of Optuna Study\")\n",
    "\n",
    "    # Add a color bar for grayscale objective values\n",
    "    sm = plt.cm.ScalarMappable(cmap=\"gray\", norm=plt.Normalize(vmin=df[objective_column].min(), vmax=df[objective_column].max()))\n",
    "    sm.set_array(objective_values.values)\n",
    "    cbar = plt.colorbar(sm, ax=plt.gca(), pad=0.01)  # Reduced padding to move the colorbar closer\n",
    "    cbar.set_label(\"RMSE averaged over 3 CV folds\")\n",
    "\n",
    "\n",
    "    # Add min and max values for each parameter near x-axis labels\n",
    "    for i, column in enumerate(parameters.columns):\n",
    "        original_min = df[column].min()\n",
    "        original_max = df[column].max()\n",
    "        ax.text(\n",
    "            i, -0.02, f\"{original_min:.2f}\", ha=\"center\", va=\"top\", fontsize=9, color=\"black\", transform=ax.transData\n",
    "        )\n",
    "        ax.text(\n",
    "            i, 1.06, f\"{original_max:.2f}\", ha=\"center\", va=\"top\", fontsize=9, color=\"black\", transform=ax.transData\n",
    "        )\n",
    "\n",
    "    # Remove y-axis and y-axis labels\n",
    "    ax.get_yaxis().set_visible(False)  # Hide y-axis\n",
    "    ax.set_ylabel(\"\")  # Remove y-axis label text\n",
    "\n",
    "    # Set transparent borders\n",
    "    ax.spines['top'].set_edgecolor((1, 1, 1, 0))\n",
    "    ax.spines['right'].set_edgecolor((1, 1, 1, 0))\n",
    "    ax.spines['left'].set_edgecolor((1, 1, 1, 0))\n",
    "    ax.spines['bottom'].set_edgecolor((1, 1, 1, 0))\n",
    "\n",
    "    # Set x and y ticks transparent\n",
    "    ax.tick_params(axis='x', color=(1, 1, 1, 0))\n",
    "    ax.tick_params(axis='y', color=(1, 1, 1, 0))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def load_and_plot_horizontal_histograms_(outdir: str):\n",
    "    \"\"\"\n",
    "    Load the complete study results from a CSV file and create horizontal histograms for each parameter.\n",
    "\n",
    "    Args:\n",
    "        outdir (str): Path to the directory containing the study results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the data\n",
    "    complete_results_path = os.path.join(outdir, '_complete_study_results.csv')\n",
    "    if not os.path.isfile(complete_results_path):\n",
    "        raise FileNotFoundError(f\"Complete study results file not found: {complete_results_path}\")\n",
    "\n",
    "    df = pd.read_csv(complete_results_path)\n",
    "\n",
    "    # Drop irrelevant columns\n",
    "    if 'state' in df.columns:\n",
    "        df = df[df['state'] == 'COMPLETE']  # Only consider completed trials\n",
    "        df.drop(columns=['state'], inplace=True)\n",
    "\n",
    "    # Determine the best parameter values\n",
    "    best_params = df.loc[df['value'].idxmin()] if 'value' in df.columns else None\n",
    "\n",
    "    df.drop(columns=['number', 'value'], inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "    # Plot horizontal histograms for each parameter\n",
    "    num_params = len(df.columns)\n",
    "    fig, axes = plt.subplots(1, num_params, figsize=(8, 3), sharey=False)\n",
    "\n",
    "    if num_params == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable if there's only one parameter\n",
    "\n",
    "    for ax, column in zip(axes, df.columns):\n",
    "        data = df[column].dropna()\n",
    "        # Determine the number of bins based on the number of samples, limited to a maximum\n",
    "        num_bins = min(50, max(5, int(len(data) ** 0.5)))\n",
    "\n",
    "        counts, bins, patches = ax.hist(data, bins=num_bins, orientation='horizontal', color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        ax.set_ylabel(column, fontsize=9)  # Move the column name to the y-axis label\n",
    "\n",
    "        # Find the largest bin and annotate it\n",
    "        max_count_idx = counts.argmax()\n",
    "        max_count = counts[max_count_idx]\n",
    "        bin_center = (bins[max_count_idx] + bins[max_count_idx + 1]) / 2\n",
    "        # ax.text(max_count, bin_center, str(int(max_count)), ha='left', va='center', fontsize=8, color='black')\n",
    "\n",
    "        # Add a horizontal line at the best parameter value if available\n",
    "        if best_params is not None and column in best_params:\n",
    "            best_value = best_params[column]\n",
    "            ax.axhline(y=best_value, color='black', linestyle='-', linewidth=2)#, label='Best Value')\n",
    "            # ax.legend(fontsize=8)\n",
    "\n",
    "        # Remove x-axis ticks and labels\n",
    "        ax.set_xticks([])\n",
    "\n",
    "        # Remove axis lines\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def load_and_plot_horizontal_histograms(outdir: str):\n",
    "    \"\"\"\n",
    "    Load the complete study results from a CSV file and create horizontal histograms for each parameter.\n",
    "\n",
    "    Args:\n",
    "        outdir (str): Path to the directory containing the study results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the data\n",
    "    complete_results_path = os.path.join(outdir, '_complete_study_results.csv')\n",
    "    if not os.path.isfile(complete_results_path):\n",
    "        raise FileNotFoundError(f\"Complete study results file not found: {complete_results_path}\")\n",
    "\n",
    "    df = pd.read_csv(complete_results_path)\n",
    "\n",
    "    # Drop irrelevant columns\n",
    "    if 'state' in df.columns:\n",
    "        df = df[df['state'] == 'COMPLETE']  # Only consider completed trials\n",
    "        df.drop(columns=['state'], inplace=True)\n",
    "\n",
    "    # Determine the best parameter values\n",
    "    best_params = df.loc[df['value'].idxmin()] if 'value' in df.columns else None\n",
    "\n",
    "    df.drop(columns=['number', 'value'], inplace=True, errors='ignore')\n",
    "\n",
    "    # Plot horizontal histograms for each parameter\n",
    "    num_params = len(df.columns)\n",
    "    fig, axes = plt.subplots(1, num_params, figsize=(8, 3), sharey=False)\n",
    "\n",
    "    if num_params == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable if there's only one parameter\n",
    "\n",
    "    for ax, column in zip(axes, df.columns):\n",
    "        data = df[column].dropna()\n",
    "        # Determine the number of bins based on the number of samples, limited to a maximum\n",
    "        num_bins = min(50, max(5, int(len(data) ** 0.5)))\n",
    "\n",
    "        counts, bins, patches = ax.hist(\n",
    "            data, bins=num_bins, orientation='horizontal', color='gray', edgecolor='black', alpha=0.6\n",
    "        )\n",
    "        ax.set_ylabel(column, fontsize=9)  # Move the column name to the y-axis label\n",
    "\n",
    "        # Check for insufficient exploration (low variance or concentration near bounds)\n",
    "        is_low_variance = data.std() < (data.max() - data.min()) * 0.1\n",
    "        is_edge_concentrated = (data < data.min() + (data.max() - data.min()) * 0.1).mean() > 0.5 or \\\n",
    "                               (data > data.max() - (data.max() - data.min()) * 0.1).mean() > 0.5\n",
    "        if is_low_variance or is_edge_concentrated:\n",
    "            for patch in patches:\n",
    "                patch.set_hatch('////')  # Add hashing to the histogram boxes\n",
    "\n",
    "        # Find the largest bin and annotate it\n",
    "        max_count_idx = counts.argmax()\n",
    "        max_count = counts[max_count_idx]\n",
    "        bin_center = (bins[max_count_idx] + bins[max_count_idx + 1]) / 2\n",
    "        # ax.text(max_count, bin_center, str(int(max_count)), ha='left', va='center', fontsize=8, color='black')\n",
    "\n",
    "        # Add a horizontal line at the best parameter value if available\n",
    "        if best_params is not None and column in best_params:\n",
    "            best_value = best_params[column]\n",
    "            ax.axhline(y=best_value, color='black', linestyle='-', linewidth=2)#, label='Best Value')\n",
    "            # ax.legend(fontsize=8)\n",
    "\n",
    "\n",
    "        # Remove x-axis ticks and labels\n",
    "        ax.set_xticks([])\n",
    "        ax.grid(color='white')\n",
    "\n",
    "        # Remove axis lines\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "    fig.suptitle(f\"Optuna Study Results. Best trial {int(best_params['number'])}/{len(df)} with RMSE={best_params['value']:.1f}\", fontsize=12, y=0.92)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./optuna_study_result.png', dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path = TaskPaths(target=target, model_label='XGBoost',working_dir='../forecasting_modules/output/',verbose=True)\n",
    "# plot_optuna_results(outdir=path.to_finetuned())\n",
    "# plot_optuna_study_grid(outdir=path.to_finetuned())\n",
    "# load_and_plot_parallel_coordinates(outdir=path.to_finetuned())\n",
    "load_and_plot_horizontal_histograms(outdir=path.to_finetuned())\n",
    "\n",
    "# Example usage:\n",
    "# plot_optuna_results('path_to_outdir/')"
   ],
   "id": "88e73d31dba53840",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "w# Split Data Into Hist and Forecast and Combine With SMARD Data",
   "id": "c3c6fc1eb70eeecb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize the split between data\n",
    "if cutoff == cutoff.normalize():\n",
    "    print(\"The cutoff timestamp corresponds to the beginning of the day.\")\n",
    "print(cutoff)\n",
    "df_om_prep[:cutoff-timedelta(hours=1)].tail(30*24)['wind_speed_10m_hsee'].plot()\n",
    "df_om_prep[cutoff:].head(24)['wind_speed_10m_hsee'].plot()\n",
    "plt.axvline(cutoff)"
   ],
   "id": "39bd9685d1d21729",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def handle_nans_with_interpolation(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Checks each column of the DataFrame for NaNs. If a column has more than 3 consecutive NaNs,\n",
    "    it raises a ValueError. Otherwise, fills the NaNs using bi-directional interpolation.\n",
    "    \"\"\"\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    def check_consecutive_nans(series: pd.Series):\n",
    "        # Identify consecutive NaNs by grouping non-NaN segments and counting consecutive NaNs\n",
    "        consecutive_nans = (series.isna().astype(int)\n",
    "                            .groupby((~series.isna()).cumsum())\n",
    "                            .cumsum())\n",
    "        if consecutive_nans.max() > 3:\n",
    "            raise ValueError(f\"Column '{series.name}' in {name} contains more than 3 consecutive NaNs.\")\n",
    "\n",
    "    # Check all columns for consecutive NaNs first\n",
    "    for col in df_copy.columns:\n",
    "        check_consecutive_nans(df_copy[col])\n",
    "\n",
    "    # Interpolate all columns at once after confirming they're valid\n",
    "    df_copy = df_copy.interpolate(method='linear', limit_direction='both', axis=0)\n",
    "\n",
    "    return df_copy\n",
    "def fix_broken_periodicity_with_interpolation(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fixes broken hourly periodicity by adding missing timestamps if fewer than 3 consecutive are missing.\n",
    "    Raises an error if more than 3 consecutive timestamps are missing.\n",
    "    Missing values are filled using time-based interpolation.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(f\"The DataFrame {name} must have a datetime index.\")\n",
    "\n",
    "    expected_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\n",
    "    missing_timestamps = expected_index.difference(df.index)\n",
    "\n",
    "    if missing_timestamps.empty:\n",
    "        print(f\"The DataFrame {name} is already hourly with no missing segments.\")\n",
    "        return df\n",
    "\n",
    "    # Convert to a Series to check consecutive missing timestamps\n",
    "    missing_series = pd.Series(missing_timestamps)\n",
    "    groups = (missing_series.diff() != pd.Timedelta(hours=1)).cumsum()\n",
    "\n",
    "    # Check if any group has more than 3 missing points\n",
    "    group_counts = groups.value_counts()\n",
    "    if (group_counts > 3).any():\n",
    "        bad_group = group_counts[group_counts > 3].index[0]\n",
    "        raise ValueError(f\"More than 3 consecutive missing timestamps detected: \"\n",
    "                         f\"{missing_series[groups == bad_group].values} in {name}\")\n",
    "\n",
    "    # Reindex and interpolate\n",
    "    fixed_df = df.reindex(expected_index)\n",
    "    fixed_df = fixed_df.interpolate(method='time')\n",
    "\n",
    "    print(f\"Added and interpolated {len(missing_timestamps)} missing timestamps in {name}.\")\n",
    "\n",
    "    return fixed_df\n",
    "def validate_dataframe(df: pd.DataFrame, name: str = '') -> pd.DataFrame:\n",
    "    \"\"\"Check for NaNs, missing values, and periodicity in a time-series DataFrame.\"\"\"\n",
    "\n",
    "    # Check for NaNs\n",
    "    if df.isnull().any().any():\n",
    "        print(f\"ERROR! {name} DataFrame contains NaN values.\")\n",
    "        df = handle_nans_with_interpolation(df, name)\n",
    "\n",
    "    # Check if index is sorted in ascending order\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        print(f\"ERROR! {name} The index is not in ascending order.\")\n",
    "        raise ValueError(\"Data is not in ascending order.\")\n",
    "\n",
    "    # Check for hourly frequency with no missing segments\n",
    "    full_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\n",
    "    if not full_range.equals(df.index):\n",
    "        print(f\"ERROR! {name} The data is not hourly or has missing segments.\")\n",
    "        df = fix_broken_periodicity_with_interpolation(df, name)\n",
    "\n",
    "    return df\n",
    "\n",
    "horizon = 7 * 24\n",
    "# merger with SMRD target column\n",
    "target = 'wind_offshore'\n",
    "df_om_prep.dropna(inplace=True, how='any')\n",
    "df_hist = pd.merge(\n",
    "    df_om_prep[:cutoff-timedelta(hours=1)],\n",
    "    df_smard[:cutoff-timedelta(hours=1)][target],\n",
    "    left_index=True, right_index=True, how=\"inner\"\n",
    ")\n",
    "df_forecast = df_om_prep[cutoff : cutoff+timedelta(hours=horizon - 1)]\n",
    "df_hist = validate_dataframe(df_hist, 'df_hist')\n",
    "df_forecast = validate_dataframe(df_forecast, 'df_forecast')\n",
    "df_hist = df_hist[df_hist.index[-1]-pd.Timedelta(hours = 100 * horizon - 1):]\n",
    "print(f\"Features {len(df_hist.columns)-1} hist.shape={df_hist.shape} ({int(len(df_hist)/horizon)}) forecast.shape={df_forecast.shape}\")\n",
    "print(f\"Hist: from {df_hist.index[0]} to {df_hist.index[-1]} ({len(df_hist)/horizon})\")\n",
    "print(f\"Fore: from {df_forecast.index[0]} to {df_forecast.index[-1]} ({len(df_forecast)/horizon})\")\n",
    "\n",
    "outdir = \"../tmp_database/\"\n",
    "df_hist.to_parquet(f\"{outdir}history.parquet\")\n",
    "df_forecast.to_parquet(f\"{outdir}forecast.parquet\")"
   ],
   "id": "eaf4819d14851387",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(df_hist.shape, df_forecast.shape)",
   "id": "96a7f2f7aaabcea9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5a55b2c010e0267c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualize",
   "id": "d6e4a5b6227ac86e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4075b868a5de2007",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from forecasting_modules import compute_timeseries_split_cutoffs, compute_error_metrics\n",
    "\n",
    "cutoffs = compute_timeseries_split_cutoffs(\n",
    "    df_hist.index,\n",
    "    horizon=len(df_forecast.index),\n",
    "    delta=len(df_forecast.index),\n",
    "    folds=5,\n",
    "    min_train_size=30*24\n",
    ")\n",
    "smard_res = []\n",
    "results = {}\n",
    "smard_metrics = {}\n",
    "for i, cutoff in enumerate(cutoffs):\n",
    "    mask = (df_hist.index >= cutoff) & (df_hist.index < cutoff + pd.Timedelta(hours=len(df_forecast.index)))\n",
    "    mask_ = (df_smard.index >= cutoff) & (df_smard.index < cutoff + pd.Timedelta(hours=len(df_forecast.index)))\n",
    "    actual = df_hist[target][mask]\n",
    "    predicted = df_smard[f\"{target}_forecasted\"][mask_]\n",
    "    # print(f\"{predicted.index[0]}\")\n",
    "    # print(f\"\\t{predicted.index[-1]}\")\n",
    "    df = pd.DataFrame({\n",
    "        f'{target}_actual':actual.values,\n",
    "        f'{target}_fitted': predicted.values,\n",
    "        f'{target}_lower': np.zeros_like(actual.values),\n",
    "        f'{target}_upper': np.zeros_like(actual.values)\n",
    "    }, index=actual.index)\n",
    "    smard_res.append(copy.deepcopy(df))\n",
    "    smard_metrics[cutoff] = compute_error_metrics(target, df)\n",
    "\n",
    "smard_metrics_ = [smard_metrics[key] for key in smard_metrics.keys()]\n",
    "ave_metrics = {\n",
    "    key: np.mean( [smard_metrics_[i][key] for i in range(len((smard_metrics_)))] ) for key in list(smard_metrics_[0].keys())\n",
    "}\n",
    "smard_metrics_.append(ave_metrics)\n"
   ],
   "id": "4f937ae0cfe5bf56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "working_dir = f'../forecasting_modules/output/'\n",
    "from forecasting_modules import ForecastingTaskSingleTarget\n",
    "# Load our past and current forecasts\n",
    "forecast_res:dict = ForecastingTaskSingleTarget._load_trained_model(\n",
    "    target=target,\n",
    "    model_label='ensemble[ElasticNet](XGBoost,ElasticNet)',#\"ensemble[ElasticNet](XGBoost,ElasticNet)\",#'ensemble[XGBoost](XGBoost,ElasticNet)',\n",
    "    working_dir=working_dir,\n",
    "    train_forecast='train',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "n = 4 # number of windows to show\n",
    "\n",
    "\n",
    "smard_forecast:pd.DataFrame = copy.deepcopy(smard_res[-1])\n",
    "smard_forecast[:] = 0 # unknown, as SMARD does not provide week-ahead forecast from now\n",
    "tasks = [\n",
    "    {'model':'SMARD','n':n,'name':'TSO day-ahead forecast','lw':1.0,'color':'blue','ci_alpha':0.0,\n",
    "     'results':smard_res[-n:],'metrics':smard_metrics_[-n-1:],'forecast':None,'ahead':'day'}, #\n",
    "    {'model':'XGBoost','n':n,'name':'Our week-ahead forecast','lw':1.0,'color':'red','ci_alpha':0.0,\n",
    "     'results':forecast_res['results'][-n:],'metrics':forecast_res['metrics'][-n-1:],'forecast':None,'ahead':'week'} # forecast_res['forecast']\n",
    "]\n",
    "# plot_time_series_with_residuals(\n",
    "#     tasks=tasks,\n",
    "#     target=target,\n",
    "#     ylabel='Off-shore Wind Generation',\n",
    "# )fig,a\n",
    "# fig,ax = plt.subplots()\n",
    "# smard_res[-1][f'{target}_actual'].plot(ax=ax, color='blue')\n",
    "# forecast_res['results'][-1][f'{target}_actual'].plot(ax=ax, color='red')"
   ],
   "id": "68634d57eecad44e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_metric_evolution(file_path: str, metric: str, smard_metrics:dict or None):\n",
    "    # Load the dataframe\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Filter relevant columns\n",
    "    if metric not in df.columns:\n",
    "        raise ValueError(f\"Metric '{metric}' not found in the dataframe.\")\n",
    "\n",
    "    # Sort the dataframe by horizon and convert horizon to datetime\n",
    "    df['horizon'] = pd.to_datetime(df['horizon'])\n",
    "    df = df.sort_values(by='horizon')\n",
    "\n",
    "    markers = ['s', 'o', 'v', '^', 'P']\n",
    "\n",
    "    # Plot the metric evolution\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    for marker, model_label in zip(markers, df['model_label'].unique()):\n",
    "        if model_label.__contains__('ensemble'): markerfacecolor = 'black'\n",
    "        else: markerfacecolor = 'None'\n",
    "        method_df_trained = df[(df['model_label'] == model_label) & (df['method'] == 'trained')]\n",
    "        ax.plot(method_df_trained['horizon'], method_df_trained[metric], linestyle='None',\n",
    "                marker=marker, label=model_label, color='black',\n",
    "                markerfacecolor=markerfacecolor, markeredgecolor='black', markersize=8\n",
    "                )\n",
    "        # method_df_forecast = df[(df['model_label'] == model_label) & (df['method'] == 'trained')]\n",
    "        # ax.plot(method_df_forecast['horizon'], method_df_forecast[metric], linestyle='None', marker=marker)\n",
    "    if smard_metrics is not None:\n",
    "        print(smard_metrics.keys())\n",
    "        ax.plot(\n",
    "            pd.to_datetime(list(smard_metrics.keys())),\n",
    "            [smard_metric[metric] for smard_metric in smard_metrics.values()],\n",
    "            linestyle='None',\n",
    "            marker='_',\n",
    "            label='SMARD (day-ahead)',\n",
    "            color='black',\n",
    "            markerfacecolor='black',\n",
    "            markeredgecolor='black',\n",
    "            markersize=8\n",
    "        )\n",
    "    # Set x-ticks at unique horizon values\n",
    "    unique_horizons = df['horizon'].sort_values().unique()\n",
    "    ax.set_xticks(unique_horizons)\n",
    "    ax.set_xticklabels([\n",
    "        h.strftime('%Y-%m-%d') for h in unique_horizons], rotation=0, ha='center'#'right'\n",
    "    )\n",
    "\n",
    "    ax.grid(True, linestyle='-', alpha=0.4)\n",
    "    ax.tick_params(axis='x', direction='in', bottom=True)\n",
    "    ax.tick_params(axis='y', which='both', direction='in', left=True, right=True)\n",
    "    # Set border lines transparent by setting the edge color and alpha\n",
    "    ax.spines['top'].set_edgecolor((1, 1, 1, 0))  # Transparent top border\n",
    "    ax.spines['right'].set_edgecolor((1, 1, 1, 0))  # Transparent right border\n",
    "    ax.spines['left'].set_edgecolor((1, 1, 1, 0))  # Transparent left border\n",
    "    ax.spines['bottom'].set_edgecolor((1, 1, 1, 0))  # Transparent bottom border\n",
    "\n",
    "    # Make x and y ticks transparent\n",
    "    ax.tick_params(axis='x', color=(1, 1, 1, 0))  # Transparent x ticks\n",
    "    ax.tick_params(axis='y', color=(1, 1, 1, 0))  # Transparent y ticks\n",
    "\n",
    "    # ax.xaxis.set_major_locator(mdates.DayLocator())\n",
    "    # ax_bottom.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))  # Format as \"Dec\n",
    "\n",
    "    ax.set_title(f\"{metric.upper()} for Several Out-of-Sample Forecasts\")\n",
    "    ax.set_xlabel(\"Starting Date of the Forecasting Horizon\")\n",
    "    ax.set_ylabel(f\"Horizon Averaged {metric.upper()}\")\n",
    "    ax.legend(fancybox=False, frameon=False)\n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_path.replace(\".csv\",\".png\"), dpi=600)\n",
    "    plt.show()\n",
    "def plot_metric_evolutions(file_path: str, metrics: list, smard_metrics:dict or None):\n",
    "    # Load the dataframe\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Filter relevant columns\n",
    "    for metric in metrics:\n",
    "        if metric not in df.columns:\n",
    "            raise ValueError(f\"Metric '{metric}' not found in the dataframe.\")\n",
    "\n",
    "    # Sort the dataframe by horizon and convert horizon to datetime\n",
    "    df['horizon'] = pd.to_datetime(df['horizon'])\n",
    "    df = df.sort_values(by='horizon')\n",
    "\n",
    "    markers = ['s', 'o', 'v', '^', 'P']\n",
    "\n",
    "    # Plot the metric evolution\n",
    "    fig, axes = plt.subplots(figsize=(8, 2*len(metrics)+1), ncols=1, nrows=len(metrics), sharex=True, sharey=False)\n",
    "    if not hasattr(axes, '__len__'): axes = [axes]\n",
    "    for i, (metric, ax) in enumerate(zip(metrics, axes)):\n",
    "        for marker, model_label in zip(markers, df['model_label'].unique()):\n",
    "            if model_label.__contains__('ensemble'): markerfacecolor = 'black'\n",
    "            else: markerfacecolor = 'None'\n",
    "            method_df_trained = df[(df['model_label'] == model_label) & (df['method'] == 'trained')]\n",
    "            ax.plot(method_df_trained['horizon'], method_df_trained[metric], linestyle='None',\n",
    "                    marker=marker, label=model_label, color='black',\n",
    "                    markerfacecolor=markerfacecolor, markeredgecolor='black', markersize=8\n",
    "                    )\n",
    "            # method_df_forecast = df[(df['model_label'] == model_label) & (df['method'] == 'trained')]\n",
    "            # ax.plot(method_df_forecast['horizon'], method_df_forecast[metric], linestyle='None', marker=marker)\n",
    "        if smard_metrics is not None:\n",
    "            print(smard_metrics.keys())\n",
    "            ax.plot(\n",
    "                pd.to_datetime(list(smard_metrics.keys())),\n",
    "                [smard_metric[metric] for smard_metric in smard_metrics.values()],\n",
    "                linestyle='None',\n",
    "                marker='_',\n",
    "                label='SMARD (day-ahead)',\n",
    "                color='black',\n",
    "                markerfacecolor='black',\n",
    "                markeredgecolor='black',\n",
    "                markersize=8\n",
    "            )\n",
    "        # Set x-ticks at unique horizon values\n",
    "        unique_horizons = df['horizon'].sort_values().unique()\n",
    "        ax.set_xticks(unique_horizons)\n",
    "        ax.set_xticklabels([\n",
    "            h.strftime('%Y-%m-%d') for h in unique_horizons], rotation=0, ha='center'#'right'\n",
    "        )\n",
    "\n",
    "        ax.grid(True, linestyle='-', alpha=0.4)\n",
    "        ax.tick_params(axis='x', direction='in', bottom=True)\n",
    "        ax.tick_params(axis='y', which='both', direction='in', left=True, right=True)\n",
    "        # Set border lines transparent by setting the edge color and alpha\n",
    "        ax.spines['top'].set_edgecolor((1, 1, 1, 0))  # Transparent top border\n",
    "        ax.spines['right'].set_edgecolor((1, 1, 1, 0))  # Transparent right border\n",
    "        ax.spines['left'].set_edgecolor((1, 1, 1, 0))  # Transparent left border\n",
    "        ax.spines['bottom'].set_edgecolor((1, 1, 1, 0))  # Transparent bottom border\n",
    "\n",
    "        # Make x and y ticks transparent\n",
    "        ax.tick_params(axis='x', color=(1, 1, 1, 0))  # Transparent x ticks\n",
    "        ax.tick_params(axis='y', color=(1, 1, 1, 0))  # Transparent y ticks\n",
    "\n",
    "        # ax.xaxis.set_major_locator(mdates.DayLocator())\n",
    "        # ax_bottom.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))  # Format as \"Dec\n",
    "        if i == 0:\n",
    "            ax.set_title(f\"{metric.upper()} for Several Out-of-Sample Forecasts\")\n",
    "        if i == len(metrics)-1:\n",
    "            ax.legend(fancybox=False, frameon=False,loc='upper right')\n",
    "            ax.set_xlabel(\"Starting Date of the Forecasting Horizon\")\n",
    "        ax.set_ylabel(f\"Horizon Averaged {metric.upper()}\")\n",
    "        ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_path.replace(\".csv\",\".png\"), dpi=600)\n",
    "    plt.show()\n",
    "# plot_metric_evolution(working_dir+target+'/summary_metrics.csv', metric='rmse', smard_metrics=smard_metrics)\n",
    "plot_metric_evolutions(working_dir+target+'/summary_metrics.csv', metrics=['rmse','smape'], smard_metrics=smard_metrics)"
   ],
   "id": "3671585021457fd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_time_series_with_residuals(tasks: list[dict], target: str = 'total_grid_load', ylabel: str = '',**kwargs):\n",
    "    '''\n",
    "\n",
    "    Plots forecasts split into adjacent windows. Each window can show several forecasts performed with\n",
    "    different methods listed in 'tasks', where each entry is a dictionary with 'results', 'metrics' and 'forecast'.\n",
    "    Last panel shows the latest forecast (as given in 'forecast' in tasks) and last metrics from 'metrics' in tasks.\n",
    "    Bottom panels show residuals between actual target variable and forecasted.\n",
    "\n",
    "    :param tasks: list[dict] where each dict represents a model's forecasting results that consist of\n",
    "    - tasks[0]['results']:list[pd.DataFrame] list of forecasts for past forecasting winows where each dataframe has:\n",
    "    f'{target}_actual', f'{target}_fitted', f'{target}_lower', 'f'{target}_upper'\n",
    "    - tasks[0]['metrics]:list[dict] list of performance metrics for each forecasted window (RMSE,sMAPE...) where the last\n",
    "    element in the list contains the average metrics\n",
    "    tasks[0]['forecast']:pd.Dataframe -- same as dataframes in 'results' but with the current latest forecast\n",
    "    :param target: str target name\n",
    "    :param label: y-label for the top panels\n",
    "    :param kwargs: additional arguments for plotting\n",
    "    :return: None\n",
    "    '''\n",
    "\n",
    "    plot_residuals = False\n",
    "    if 'residuals' in kwargs:\n",
    "        plot_residuals = kwargs['residuals']\n",
    "\n",
    "    legends_per_panel = False\n",
    "    if 'legends_per_panel' in kwargs:\n",
    "        legends_per_panel = kwargs['legends_per_panel']\n",
    "\n",
    "    label_errs = True\n",
    "    if 'label_errs' in kwargs:\n",
    "        label_errs = kwargs['label_errs']\n",
    "\n",
    "    if 'watermark' in kwargs:\n",
    "        watermark_text = kwargs['watermark']\n",
    "    else:\n",
    "        watermark_text = None\n",
    "\n",
    "    if 'itime' in kwargs:\n",
    "        itime = kwargs['itime']\n",
    "    else:\n",
    "        itime = None\n",
    "\n",
    "    # Determine the maximum number of results across tasks\n",
    "    plot_forecast = False\n",
    "    for task in tasks:\n",
    "        if task['forecast']:\n",
    "            plot_forecast = True\n",
    "\n",
    "    max_n_results = max(len(task['results']) for task in tasks)\n",
    "    if plot_forecast: n_cols = max_n_results + 1  # Plus one for 'forecast'\n",
    "    else: n_cols = max_n_results\n",
    "\n",
    "\n",
    "    if not 'drawstyle' in kwargs: drawstyle='default'\n",
    "    else: drawstyle=kwargs['drawstyle']\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows = 2 if plot_residuals else 1, ncols=n_cols,\n",
    "        figsize=kwargs['figsize'] if 'figsize' in kwargs else (n_cols * 5, 8),\n",
    "        gridspec_kw={\n",
    "            'height_ratios': [3, 1] if plot_residuals else [1],\n",
    "            'hspace': 0.02, 'wspace': 0.01\n",
    "        },\n",
    "        sharex='col', sharey='row'\n",
    "    )\n",
    "\n",
    "    # Define column names\n",
    "    actual_col = f'{target}_actual'\n",
    "    fitted_col = f'{target}_fitted'\n",
    "    lower_col = f'{target}_lower'\n",
    "    upper_col = f'{target}_upper'\n",
    "\n",
    "    # For each column\n",
    "\n",
    "    for i in range(n_cols):\n",
    "        ax_top = axes[0, i] if plot_residuals else axes[i]\n",
    "        ax_bottom = axes[1, i] if plot_residuals else None\n",
    "\n",
    "        # Flag to check if 'Actual' data has been plotted\n",
    "        actual_plotted = False\n",
    "\n",
    "        # For each task\n",
    "        for task in tasks:\n",
    "            name = task['name']\n",
    "            color = task['color']\n",
    "            ci_alpha=task['ci_alpha']\n",
    "            lw=task['lw']\n",
    "            # Determine if the task has data for this column\n",
    "            if i < len(task['results']):\n",
    "                df = task['results'][i]\n",
    "                errs = task['metrics'][i]\n",
    "            elif i == max_n_results:\n",
    "                df = task['forecast']\n",
    "                errs = task['metrics'][i]\n",
    "            else:\n",
    "                continue  # Skip plotting for this task in this column\n",
    "\n",
    "            if itime is not None:\n",
    "                itime_:int = itime - i * len(df)\n",
    "                if itime_ > len(df) - 1: itime_ = len(df)-1\n",
    "                if itime_ < 0: itime_ = 0\n",
    "                mask_actual = df.index <= df.index[itime_]\n",
    "\n",
    "                if task['ahead'] == 'day': itime__ = min(itime_+24 if itime_>0 else 0, len(df)-1)\n",
    "                elif task['ahead'] == 'week': itime__ = min(itime_+7*24 if itime_>0 else 0, len(df)-1)\n",
    "                else:raise KeyError(f'Unrecognized forecast model for index offset')\n",
    "                mask_forecast = df.index <= df.index[itime__]\n",
    "            else:\n",
    "                mask_actual = df.index <= df.index[-1]\n",
    "                mask_forecast = df.index <= df.index[-1]\n",
    "\n",
    "            # Plot 'Actual' data once per subplot\n",
    "            if not actual_plotted : #and i != n_cols-1:\n",
    "                ax_top.plot(df[mask_actual].index, df[mask_actual][actual_col],\n",
    "                            label='Actual', color='black', drawstyle=drawstyle, lw=1.5)\n",
    "                actual_plotted = True\n",
    "\n",
    "            # Plot fitted data\n",
    "            if label_errs and (errs is not None and i != n_cols-1):\n",
    "                label = name + ' '  fr\"RMSE={errs['rmse']:.1f}\" + fr\" sMAPE={errs['smape']:.1f}\"\n",
    "            elif label_errs:\n",
    "                label = name + ' ' fr\"$\\langle$RMSE$\\rangle$={errs['rmse']:.1f}\" \\\n",
    "                         + fr\" $\\langle$sMAPE$\\rangle$={errs['smape']:.1f}\"\n",
    "            else:\n",
    "                label = name\n",
    "            ax_top.plot(df[mask_forecast].index, df[mask_forecast][fitted_col],\n",
    "                        label=label, color=color, drawstyle=drawstyle, lw=lw)\n",
    "\n",
    "            # Plot confidence intervals\n",
    "            if ci_alpha > 0.:\n",
    "                ax_top.fill_between(\n",
    "                    df[mask_forecast].index, df[mask_forecast][lower_col], df[mask_forecast][upper_col],\n",
    "                    color=color, alpha=ci_alpha\n",
    "                )\n",
    "\n",
    "            # Plot residuals in the bottom panel\n",
    "            residuals = (df[actual_col] - df[fitted_col]) #/ df[actual_col]\n",
    "            if ax_bottom: ax_bottom.plot(df[mask_actual].index, residuals[mask_actual], label=name, color=color, drawstyle=drawstyle, lw=lw)\n",
    "\n",
    "            # limit plots\n",
    "            ax_top.set_xlim(df.index.min(),df.index.max())\n",
    "            if ax_bottom: ax_bottom.set_xlim(df.index.min(),df.index.max())\n",
    "            if 'ylim0' in kwargs: ax_top.set_ylim(kwargs['ylim0'][0], kwargs['ylim0'][1])\n",
    "            if 'ylim1' in kwargs and ax_bottom: ax_bottom.set_ylim(kwargs['ylim1'][0], kwargs['ylim1'][1])\n",
    "\n",
    "        # print(f\"N={len(df.index)} idx0={df.index[0].isoformat()}\")\n",
    "        # Add a horizontal line at y=0 in residuals plot\n",
    "        if ax_bottom: ax_bottom.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "        # Set titles and labels\n",
    "        if itime and itime_ > 0:\n",
    "            if i < max_n_results:\n",
    "                ax_top.set_title(f'Week {df.index[-1].isocalendar().week} of 2024', fontsize=14, weight='bold')\n",
    "            else:\n",
    "                ax_top.set_title('Current Forecast', fontsize=14, weight='bold')\n",
    "\n",
    "\n",
    "        if ylabel and i == 0:\n",
    "            ax_top.set_ylabel(ylabel)\n",
    "            if ax_bottom: ax_bottom.set_ylabel('Residual / Actual')\n",
    "\n",
    "        # legend in the empty area in residual plots\n",
    "        if i == n_cols - 1 and ax_bottom:\n",
    "            ax_bottom.legend(loc='upper left', ncol=1, fontsize=10)\n",
    "\n",
    "        if legends_per_panel or i==0:#i == n_cols - 1:\n",
    "            ax_top.legend(loc='lower left', ncol=1, fontsize=10)\n",
    "\n",
    "        if watermark_text and i == n_cols-1:\n",
    "            ax_top.text(0.1, 0.1, watermark_text, transform=ax_top.transAxes,\n",
    "                    fontsize=40, color='gray', alpha=0.2,\n",
    "                    ha='center', va='center', rotation=0)\n",
    "\n",
    "        for ax in [ax_top, ax_bottom] if plot_residuals else [ax_top]:\n",
    "            ax.grid(True, linestyle='-', alpha=0.4)\n",
    "            ax.tick_params(axis='x', direction='in', bottom=True)\n",
    "            ax.tick_params(axis='y', which='both', direction='in', left=True, right=True)\n",
    "            # Set border lines transparent by setting the edge color and alpha\n",
    "            ax.spines['top'].set_edgecolor((1, 1, 1, 0))  # Transparent top border\n",
    "            ax.spines['right'].set_edgecolor((1, 1, 1, 0))  # Transparent right border\n",
    "            ax.spines['left'].set_edgecolor((1, 1, 1, 0))  # Transparent left border\n",
    "            ax.spines['bottom'].set_edgecolor((1, 1, 1, 0))  # Transparent bottom border\n",
    "\n",
    "            # Make x and y ticks transparent\n",
    "            ax.tick_params(axis='x', color=(1, 1, 1, 0))  # Transparent x ticks\n",
    "            ax.tick_params(axis='y', color=(1, 1, 1, 0))  # Transparent y ticks\n",
    "\n",
    "            # Make x and y tick labels transparent\n",
    "            # for tick_label in ax.get_xticklabels():\n",
    "            #     tick_label.set_color((1, 1, 1, 0))  # Transparent x tick labels\n",
    "            # for tick_label in ax.get_yticklabels():\n",
    "            #     tick_label.set_color((1, 1, 1, 0))  # Transparent y tick labels\n",
    "\n",
    "\n",
    "        # Improve x-axis formatting\n",
    "        # ax_bottom.set_xlabel(f'Date (month-day for $2024$)', fontsize=12)\n",
    "        if ax_bottom:\n",
    "            ax_bottom.xaxis.set_major_locator(mdates.DayLocator())\n",
    "            # ax_bottom.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "            ax_bottom.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))  # Format as \"Dec 15\"\n",
    "        else:\n",
    "            ax_top.xaxis.set_major_locator(mdates.DayLocator())\n",
    "            # ax_bottom.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "            ax_top.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))  # Format as \"Dec 15\"\n",
    "        fig.autofmt_xdate(rotation=45)\n",
    "\n",
    "\n",
    "    model_names = \"\".join(task[\"name\"]+'_' for task in tasks)\n",
    "    if 'fpath' in kwargs:\n",
    "        plt.savefig(kwargs['fpath'], bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f'{target}_{model_names}.png', bbox_inches='tight')\n",
    "    if 'show' in kwargs and kwargs['show']: plt.show()\n",
    "\n",
    "    plt.close(fig)\n",
    "    gc.collect()\n",
    "\n",
    "# Usage: Just call this function similarly to your existing plotting call\n",
    "plot_time_series_with_residuals(\n",
    "    tasks, target=target, ylabel='Offshore Wind Power',\n",
    "    residuals = False, legends_per_panel=False, label_errs=False, figsize=(12,4),\n",
    "    #watermark='V. Nedora'\n",
    "    show=True,\n",
    "    itime = 900, fpath = \"./results.png\" #fpath=os.getcwd()+'/'+'movie'+'/'+f\"{i:03}.png\",\n",
    ")\n",
    "\n",
    "# for i in range(901):\n",
    "#     print(f\"Plotting {i}/{901}\")\n",
    "#     plot_time_series_with_residuals(\n",
    "#         tasks, target=target, ylabel='Offshore Wind Power',\n",
    "#         residuals = False, legends_per_panel=False, label_errs=False, figsize=(12,4),\n",
    "#         #watermark='V. Nedora'\n",
    "#         show=False,\n",
    "#         itime = i, fpath=os.getcwd()+'/'+'movie'+'/'+f\"{i:03}.png\"\n",
    "#     )"
   ],
   "id": "e7edbc620824ed18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_broken_periodicity(df, target):\n",
    "    \"\"\"\n",
    "    Plots the DataFrame and highlights where periodicity is broken.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): A pandas DataFrame with a datetime index.\n",
    "    \"\"\"\n",
    "    # Ensure the index is datetime\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"The DataFrame must have a datetime index.\")\n",
    "\n",
    "    # Generate the expected hourly index\n",
    "    expected_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='h')\n",
    "\n",
    "    # Find missing or unexpected timestamps\n",
    "    missing_timestamps = expected_index.difference(df.index)\n",
    "    extra_timestamps = df.index.difference(expected_index)\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df[target], marker='.', label='Data')\n",
    "\n",
    "    # Highlight missing timestamps\n",
    "    for ts in missing_timestamps:\n",
    "        plt.axvline(ts, color='red', linestyle='--', alpha=0.7, label='Missing Period' if ts == missing_timestamps[0] else \"\")\n",
    "\n",
    "    # Highlight unexpected timestamps\n",
    "    for ts in extra_timestamps:\n",
    "        plt.axvline(ts, color='orange', linestyle='--', alpha=0.7, label='Unexpected Period' if ts == extra_timestamps[0] else \"\")\n",
    "\n",
    "    # Add legend and labels\n",
    "    plt.title(\"Data Plot with Highlighted Broken Periodicity\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def handle_nans_with_interpolation(df: pd.DataFrame, name:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Checks each column of the DataFrame for NaNs. If a column has more than 3 consecutive NaNs,\n",
    "    it raises a ValueError. Otherwise, it fills the NaNs using bi-directional interpolation.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with time series data (indexed by pd.Timestamp).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with NaNs filled using bi-directional interpolation.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any column contains more than 3 consecutive NaNs.\n",
    "    \"\"\"\n",
    "    def check_consecutive_nans(series: pd.Series):\n",
    "        # Find consecutive NaNs\n",
    "        consecutive_nans = series.isna().astype(int).groupby(series.notna().cumsum()).cumsum()\n",
    "        # Check if any sequence of NaNs exceeds 3\n",
    "        if consecutive_nans.max() > 3:\n",
    "            raise ValueError(f\"Column '{series.name}' in {name} contains more than 3 consecutive NaNs.\")\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    for column in df_copy.columns:\n",
    "        # Check if the column has more than 3 consecutive NaNs\n",
    "        check_consecutive_nans(df_copy[column])\n",
    "        # Fill NaNs using bi-directional interpolation\n",
    "        df_copy[column] = df_copy[column].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    return df_copy\n",
    "def fix_broken_periodicity_with_interpolation(df:pd.DataFrame, name:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fixes broken hourly periodicity by adding missing timestamps if fewer than 3 consecutive points are missing.\n",
    "    Raises an error if more than 3 consecutive timestamps are missing.\n",
    "    Missing values are filled using interpolation.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): A pandas DataFrame with a datetime index.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with missing points added and values interpolated where necessary.\n",
    "    \"\"\"\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(f\"The DataFrame {name} must have a datetime index.\")\n",
    "\n",
    "    # Generate the full expected hourly range\n",
    "    expected_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\n",
    "\n",
    "    # Find missing timestamps\n",
    "    missing_timestamps = expected_index.difference(df.index)\n",
    "\n",
    "    if missing_timestamps.empty:\n",
    "        print(f\"The DataFrame {name} is already hourly with no missing segments.\")\n",
    "        return df\n",
    "\n",
    "    # Identify consecutive missing groups\n",
    "    missing_diffs = missing_timestamps.to_series().diff().dt.seconds\n",
    "    consecutive_missing_groups = missing_diffs != 3600  # Check if time diff isn't exactly 1 hour\n",
    "\n",
    "    # Label consecutive groups\n",
    "    missing_timestamps_df = pd.DataFrame({'timestamp': missing_timestamps})\n",
    "    missing_timestamps_df['group'] = consecutive_missing_groups.cumsum()\n",
    "\n",
    "    # Check for groups with more than 3 missing points\n",
    "    for group_id, group in missing_timestamps_df.groupby('group'):\n",
    "        if len(group) > 3:\n",
    "            raise ValueError(f\"More than 3 consecutive missing timestamps detected: {group['timestamp'].values} in {name}\")\n",
    "\n",
    "    # Add missing timestamps back to the DataFrame\n",
    "    fixed_df = df.reindex(df.index.union(missing_timestamps).sort_values())\n",
    "\n",
    "    # Interpolate to fill missing values\n",
    "    fixed_df = fixed_df.interpolate(method='time')\n",
    "\n",
    "    print(f\"Added and interpolated {len(missing_timestamps)} missing timestamps in {name}.\")\n",
    "\n",
    "    return fixed_df\n",
    "def validate_dataframe(df, name:str=''):\n",
    "    ''' check for nans, missing values and preiodicity in a time-series dataframe '''\n",
    "    # Check for NaNs\n",
    "    has_nans = df.isnull().any().any()\n",
    "    if has_nans:\n",
    "        print(f\"ERROR! {name} DataFrame contains NaN values.\")\n",
    "        df = handle_nans_with_interpolation(df, name)\n",
    "\n",
    "    # Check if the index is in ascending order\n",
    "    index_ascending = df.index.is_monotonic_increasing\n",
    "    if not index_ascending:\n",
    "        print(f\"ERROR! {name} The index is not in ascending order.\")\n",
    "        raise ValueError(\"Data is not in ascending order.\")\n",
    "\n",
    "    # Check if the index is hourly with no missing segments\n",
    "    is_hourly = pd.date_range(start=df.index.min(), end=df.index.max(), freq='h').equals(df.index)\n",
    "    if not is_hourly:\n",
    "        print(f\"ERROR! {name} The data is not hourly or has missing segments.\")\n",
    "        df = fix_broken_periodicity_with_interpolation(df, name)\n",
    "    return df\n",
    "\n",
    "# merger with SMRD target column\n",
    "target = 'wind_offshore'\n",
    "df_om_prep.dropna(inplace=True, how='any')\n",
    "df_hist = pd.merge(df_om_prep, df_smard[target], left_index=True, right_index=True, how=\"inner\")\n",
    "df_forecast:pd.DataFrame = df_om_prep[cutoff+timedelta(hours=1) : cutoff+timedelta(hours=1 + 7*24)]\n",
    "df_hist:pd.DataFrame = validate_dataframe(df_hist, 'df_hist')\n",
    "df_forecast = validate_dataframe(df_forecast, 'df_forecast')\n",
    "df_hist = df_hist[df_hist.index[-1]-pd.Timedelta(weeks=50):]\n",
    "print(f\"Features {len(df_hist.columns)-1} hist.shape={df_hist.shape} ({int(len(df_hist) / 7 / 24)} weeks) forecast.shape={df_forecast.shape}\")\n"
   ],
   "id": "9794c45e6e7a89fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_hist = pd.load",
   "id": "65ab6834767fa31f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from forecasting_modules import compute_timeseries_split_cutoffs, compute_error_metrics\n",
    "cutoffs = compute_timeseries_split_cutoffs(\n",
    "    df_hist.index,\n",
    "    horizon=len(df_forecast.index),\n",
    "    delta=len(df_forecast.index),\n",
    "    folds=5,\n",
    "    min_train_size=30*24\n",
    ")\n",
    "dfs = []\n",
    "results = {}\n",
    "metrics = []\n",
    "for i, cutoff in enumerate(cutoffs):\n",
    "    print(f\"Processing {i}/{len(cutoffs)}\")\n",
    "    mask = (df_hist.index > cutoff) & (df_hist.index <= cutoff + pd.Timedelta(hours=len(df_forecast.index)))\n",
    "    mask_ = (df_smard.index > cutoff) & (df_smard.index <= cutoff + pd.Timedelta(hours=len(df_forecast.index)))\n",
    "    actual = df_hist[target][mask]\n",
    "    predicted = df_smard[f\"{target}_forecasted\"][mask_]\n",
    "    df = pd.DataFrame({\n",
    "        f'{target}_actual':actual.values,\n",
    "        f'{target}_fitted': predicted.values,\n",
    "        f'{target}_lower': np.zeros_like(actual.values),\n",
    "        f'{target}_upper': np.zeros_like(actual.values)\n",
    "    }, index=actual.index)\n",
    "    dfs.append(copy.deepcopy(df))\n",
    "    metrics.append( compute_error_metrics(target, df) )\n",
    "\n",
    "ave_metrics = {\n",
    "    key: np.mean( [metrics[i][key] for i in range(len((metrics)))] ) for key in list(metrics[0].keys())\n",
    "}\n",
    "metrics.append(ave_metrics)\n",
    "print(len(metrics))"
   ],
   "id": "8bd2e04671d7d3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_modules import plot_time_series_with_residuals\n",
    "n = 3\n",
    "forecast = copy.deepcopy(dfs[-1])\n",
    "forecast[:] = 0\n",
    "tasks = [\n",
    "    {'model':'SMARD','n':n,'name':'SMARD','lw':0.7,'color':'red','ci_alpha':0.0,\n",
    "     'results':dfs[-n:],'metrics':metrics[-n-1:],'forecast':forecast}\n",
    "]\n",
    "plot_time_series_with_residuals(\n",
    "    tasks=tasks,\n",
    "    target=target,\n",
    "    ylabel='Off-shore Wind Generation'\n",
    ")"
   ],
   "id": "7c3d62dff18453eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Preprocess SMARD data\n",
    "def aggregate_smard_data(df_smard, countries:list):\n",
    "    # 1. Creating the 'other_gen' column\n",
    "    df_smard['other_gen'] = df_smard[['biomass', 'hydropower', 'lignite', 'hard_coal', 'natural_gas',\n",
    "                                      'pumped_storage', 'other_conventional', 'other_renewables']].sum(axis=1)\n",
    "\n",
    "    # 2. Creating 'net_import' and 'net_export' columns\n",
    "    import_columns = [f\"{country}_import\" for country in countries]\n",
    "    export_columns = [f\"{country}_export\" for country in countries]\n",
    "\n",
    "    df_smard['net_import'] = df_smard[import_columns].sum(axis=1)\n",
    "    df_smard['net_export'] = df_smard[export_columns].sum(axis=1)\n",
    "\n",
    "    # 3. Selecting the required columns for the new dataframe\n",
    "    selected_columns = ['other_gen', 'net_import', 'net_export', 'total_grid_load',\n",
    "                        'residual_load', 'solar', 'wind_offshore', 'wind_onshore']\n",
    "\n",
    "    df_new = df_smard[selected_columns]\n",
    "df_smard_ = aggregate_smard_data(df_smard, list(DataEnergySMARD.country_map.keys()))"
   ],
   "id": "cd56f24ff936e38f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prepare_data_for_offshore_wind_gen(df_om, df_om_f):\n",
    "    df = df\n",
    "    location_suffix=\"_hsee\"\n",
    "    # 1. Filter for relevant location\n",
    "    cols_to_keep = [c for c in df.columns if c.endswith(location_suffix)]\n",
    "    df = df[cols_to_keep].copy()"
   ],
   "id": "93b053a6c517f0fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_df_hist_df_forecast(verbose:bool):\n",
    "    data_dir = '../database/'\n",
    "    # df = pd.read_parquet(data_dir + 'latest.parquet')\n",
    "    df_smard = pd.read_parquet(data_dir + 'smard/' + 'history.parquet')\n",
    "\n",
    "    def aggregate_smard_data(df_smard, countries:list):\n",
    "        # 1. Creating the 'other_gen' column\n",
    "        df_smard['other_gen'] = df_smard[['biomass', 'hydropower', 'lignite', 'hard_coal', 'natural_gas',\n",
    "                                          'pumped_storage', 'other_conventional', 'other_renewables']].sum(axis=1)\n",
    "\n",
    "        # 2. Creating 'net_import' and 'net_export' columns\n",
    "        import_columns = [f\"{country}_import\" for country in countries]\n",
    "        export_columns = [f\"{country}_export\" for country in countries]\n",
    "\n",
    "        df_smard['net_import'] = df_smard[import_columns].sum(axis=1)\n",
    "        df_smard['net_export'] = df_smard[export_columns].sum(axis=1)\n",
    "\n",
    "        # 3. Selecting the required columns for the new dataframe\n",
    "        selected_columns = ['other_gen', 'net_import', 'net_export', 'total_grid_load',\n",
    "                            'residual_load', 'solar', 'wind_offshore', 'wind_onshore']\n",
    "\n",
    "        df_new = df_smard[selected_columns]\n",
    "    df_smard_ = aggregate_smard_data(df_smard, list(DataEnergySMARD.country_map.keys()))\n",
    "\n",
    "    def aggregate_openmeteo_data(df_smard, locations_:list):\n",
    "\n",
    "\n",
    "\n",
    "    df_om = pd.read_parquet(data_dir + 'openmeteo/' + 'history.parquet')\n",
    "    df_om_f = pd.read_parquet(data_dir + 'openmeteo/' + 'forecast.parquet')\n",
    "    df_es = pd.read_parquet(data_dir + 'epexspot/' + 'history.parquet')\n",
    "\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"SMARD data shapes hist={df_smard.shape} start={df_smard.index[0]} end={df_smard.index[-1]}\")\n",
    "        print(f\"Openmeteo data shapes hist={df_om.shape} start={df_om.index[0]} end={df_om.index[-1]}\")\n",
    "        print(f\"Openmeteo data shapes forecast={df_om_f.shape} start={df_om_f.index[0]} end={df_om_f.index[-1]}\")\n",
    "        print(f\"EPEXSPOT data shapes hist={df_es.shape} start={df_es.index[0]} end={df_es.index[-1]}\")\n",
    "\n",
    "    df_hist = pd.merge(df_smard, df_om, left_index=True, right_index=True)\n",
    "    df_hist = pd.merge(df_hist, df_es, left_index=True, right_index=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Merged hist dataframes shape\")\n",
    "\n"
   ],
   "id": "7672e975fb194709",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_smard.tail(n=5)",
   "id": "f80b8c0f816cadba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_gen_load_with_forecasts(df_smard_):\n",
    "    \n",
    "    df_smard_ = df_smard_[~(df_smard_[['total_grid_load','residual_load','solar','wind_offshore','wind_onshore']] == 0).all(axis=1)]\n",
    "    \n",
    "    fig,axes = plt.subplots(figsize=(10,10), nrows=6, sharex='all')\n",
    "    ax = axes[0]\n",
    "    \n",
    "    ax.plot(df_smard_.index,df_smard_['total_grid_load'], color='blue', label='Actual')\n",
    "    ax.plot(df_smard_.index,df_smard_['total_grid_load_forecasted'], color='gray', label='Forecast')\n",
    "    ax.set_ylabel('Total Load')\n",
    "    \n",
    "    ax = axes[1]\n",
    "    ax.plot(df_smard_.index,df_smard_['residual_load'], color='blue', label='Actual')\n",
    "    ax.plot(df_smard_.index,df_smard_['residual_load_forecasted'], color='gray', label='Forecast')\n",
    "    ax.set_ylabel('Residual Load')\n",
    "    \n",
    "    ax = axes[2]\n",
    "    ax.plot(df_smard_.index,df_smard_['solar'], color='blue', label='Actual')\n",
    "    ax.plot(df_smard_.index,df_smard_['solar_forecasted'], color='gray', label='Forecast')\n",
    "    ax.set_ylabel('Solar Generation')\n",
    "    \n",
    "    ax = axes[3]\n",
    "    ax.plot(df_smard_.index,df_smard_['wind_offshore'], color='blue', label='Actual')\n",
    "    ax.plot(df_smard_.index,df_smard_['wind_offshore_forecasted'], color='gray', label='Forecast')\n",
    "    ax.set_ylabel('Wind Off-shore')\n",
    "    \n",
    "    ax = axes[4]\n",
    "    ax.plot(df_smard_.index,df_smard_['wind_onshore'], color='blue', label='Actual')\n",
    "    ax.plot(df_smard_.index,df_smard_['wind_onshore_forecasted'], color='gray', label='Forecast')\n",
    "    ax.set_ylabel('Wind On-shore')\n",
    "    \n",
    "    ax = axes[5]\n",
    "    ax.plot(df_smard_.index,df_smard_[['biomass','hydropower','lignite','hard_coal','natural_gas','pumped_storage','other_conventional','other_renewables']].aggregate(func='sum',axis=1), color='blue', label='Actual')\n",
    "    ax.plot(df_smard_.index,df_smard_['other_gen_forecasted'], color='gray', label='Forecast')\n",
    "    ax.set_ylabel('Other Generation')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_gen_load_with_forecasts(df_smard.tail(30*24))"
   ],
   "id": "a1d17b8e7f48dea6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_collection_modules import DataEnergySMARD\n",
    "def plot_cross_border_trade_smard(df_smard_, by_country:bool):\n",
    "    if by_country:\n",
    "        fig,axes = plt.subplots(figsize=(10,12), nrows=len(list(DataEnergySMARD.country_map.keys()))+1, sharex='all')\n",
    "    else: \n",
    "        fig,ax = plt.subplots(figsize=(10,6), nrows=1)\n",
    "        axes = [ax]\n",
    "    if by_country:\n",
    "        for ax, country in zip(axes, list(DataEnergySMARD.country_map.keys())):\n",
    "            import_ = df_smard_[f\"{country}_import\"]\n",
    "            export_ = df_smard_[f\"{country}_export\"]\n",
    "            # ax.plot(df_smard_.index,import_, color='blue', label='Import')\n",
    "            # ax.plot(df_smard_.index,export_, color='red', label='Export')\n",
    "            ax.plot(df_smard_.index,import_ + export_, color='black', label='Trade')\n",
    "            ax.set_ylabel(country)\n",
    "            ax.axhline(0,color='gray',linestyle=':')\n",
    "\n",
    "    net_import = df_smard_[[f\"{country}_import\" for country in list(DataEnergySMARD.country_map.keys())]].aggregate(func='sum',axis=1)\n",
    "    net_export = df_smard_[[f\"{country}_export\" for country in list(DataEnergySMARD.country_map.keys())]].aggregate(func='sum',axis=1)\n",
    "    \n",
    "    # Calculate moving averages\n",
    "    window_size = 24  # Adjust the window size as needed\n",
    "    net_import_ma = net_import.rolling(window=window_size).mean()\n",
    "    net_export_ma = net_export.rolling(window=window_size).mean()\n",
    "    net_sum_ma = (net_import + net_export).rolling(window=window_size).mean()\n",
    "    \n",
    "    # Plot the original data\n",
    "    axes[-1].plot(df_smard_.index, net_import, color='blue', label='Import')\n",
    "    axes[-1].plot(df_smard_.index, net_export, color='red', label='Export')\n",
    "    axes[-1].plot(df_smard_.index, net_export + net_import, color='black', label='Sum')\n",
    "    \n",
    "    # Plot the moving averages\n",
    "    axes[-1].plot(df_smard_.index, net_import_ma, color='blue', linestyle='--', label='Import (MA)')\n",
    "    axes[-1].plot(df_smard_.index, net_export_ma, color='red', linestyle='--', label='Export (MA)')\n",
    "    axes[-1].plot(df_smard_.index, net_sum_ma, color='black', linestyle='--', label='Sum (MA)')\n",
    "    \n",
    "    # Add reference line and legend\n",
    "    axes[-1].axhline(0, color='gray', linestyle=':')\n",
    "    axes[-1].legend(loc='upper right')\n",
    "    \n",
    "    # Adjust layout and show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_cross_border_trade_smard(df_smard.tail(n=30*24), by_country=False)"
   ],
   "id": "8841b5cb9252553c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# OPENMETEO",
   "id": "7a8a30b23deb2f94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_collection_modules import locations, OpenMeteo\n",
    "def plot_openmeteo(df, df_f, suffix):\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=len(list(OpenMeteo.variables_standard)), sharex='all', figsize=(10,12))\n",
    "    for ax, quantity in zip(axes, list(OpenMeteo.variables_standard)):\n",
    "        ax.plot(df.index, df[f\"{quantity}{suffix}\"], color='black', ls='-', label='Historic')\n",
    "        ax.plot(df_f.index, df_f[f\"{quantity}{suffix}\"], color='black', ls='--', label='Forecast')\n",
    "        ax.set_ylabel(quantity)\n",
    "\n",
    "        ave = df[[f\"{quantity}{loc['suffix']}\" for loc in locations]].aggregate(func='mean',axis=1)\n",
    "        ave_f = df[[f\"{quantity}{loc['suffix']}\" for loc in locations]].aggregate(func='mean',axis=1)\n",
    "\n",
    "        ax.plot(ave.index, ave, color='gray', ls='-', label='Historic')\n",
    "        ax.plot(ave_f.index, ave_f, color='gray', ls='--', label='Forecast')\n",
    "        ax.set_ylabel(quantity)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_openmeteo(df=df_om.tail(n=30*24),df_f=df_om_f,suffix='_hsee')\n",
    "    "
   ],
   "id": "1a60634e8e59d0a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_energy_data(df):\n",
    "    # Ensure datetime index\n",
    "    df_month = df.tail(30*24)\n",
    "\n",
    "    # List of countries\n",
    "    countries = ['austria', 'belgium', 'czechia', 'denmark', 'france', 'luxembourg',\n",
    "                 'netherlands', 'norway', 'poland', 'sweden', 'switzerland']\n",
    "\n",
    "    # Sum '_export' and '_import' columns for each country\n",
    "    df_flows = pd.DataFrame()\n",
    "    for country in countries:\n",
    "        df_flows[country] = df_month[f'{country}_export'] + df_month[f'{country}_import']\n",
    "\n",
    "    # Remaining columns\n",
    "    remaining_columns = ['other_gen', 'residual_load_forecast', 'solar', 'total_gen',\n",
    "                         'total_grid_load', 'wind_offshore', 'wind_onshore']\n",
    "    df_remaining = df_month[remaining_columns]\n",
    "\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(15, 10))\n",
    "\n",
    "    # First panel: stacked bar chart of df_flows\n",
    "    df_flows.plot(kind='bar', stacked=True, ax=axes[0], width=1.0, linewidth=0)\n",
    "\n",
    "    axes[0].set_title('Hourly Energy Flows by Country')\n",
    "    axes[0].set_ylabel('Total Energy Flow')\n",
    "    axes[0].legend(loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "    axes[0].set_xlabel('')\n",
    "\n",
    "    # Adjust x-axis ticks on the bar chart\n",
    "    num_ticks = len(df_flows)\n",
    "    tick_positions = range(0, num_ticks, 24)  # Show a tick every 24 bars (once per day)\n",
    "    tick_labels = df_flows.index[::24].strftime('%Y-%m-%d')\n",
    "    axes[0].set_xticks(tick_positions)\n",
    "    axes[0].set_xticklabels(tick_labels, rotation=45, ha='right')\n",
    "\n",
    "    # Second panel: lines of remaining columns\n",
    "    df_remaining.plot(ax=axes[1])\n",
    "    axes[1].set_title('Hourly Other Energy Data')\n",
    "    axes[1].set_ylabel('Value')\n",
    "    axes[1].legend(loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "\n",
    "    # Adjust x-axis ticks on the second panel\n",
    "    axes[1].xaxis.set_major_locator(mdates.DayLocator(interval=3))\n",
    "    axes[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    axes[1].set_xlabel('Date')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "visualize_energy_data(df_smard)"
   ],
   "id": "cc013b71793c0a8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "E",
   "id": "aa72c6caf175cb8d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Combined Weather and Energy",
   "id": "410f57a09b173ff9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_smard_openmeteo(df_smard, df_om, df_om_f, om_suffix):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(15, 10),sharex='all')\n",
    "    \n",
    "    ax = axes[0]\n",
    "    ax_ = ax.twinx()\n",
    "    ax.plot(df_smard.index, df_smard['solar'], color='black', label='Solar Energy Generation')\n",
    "    ax.set_ylabel('Solar Energy Generation')\n",
    "    ax_.plot(df_om.index, df_om[f'shortwave_radiation{om_suffix}'], color='red', label='Historic')\n",
    "    ax_.plot(df_om_f.index, df_om_f[f'shortwave_radiation{om_suffix}'], color='red', ls='--', label='Forecast')\n",
    "    ax_.set_ylabel('Shortwave Radiation')\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax_ = ax.twinx()\n",
    "    ax.plot(df_smard.index, df_smard['wind_onshore'], color='black', label='Solar Energy Generation')\n",
    "    ax.set_ylabel('On-shore Wind Generation')\n",
    "    ax_.plot(df_om.index, df_om[f'wind_speed_10m{om_suffix}'], color='red', label='Historic')\n",
    "    ax_.plot(df_om_f.index, df_om_f[f'wind_speed_10m{om_suffix}'], color='red', ls='--', label='Forecast')\n",
    "    ax.axvline(df_om_f.index[0])\n",
    "    ax_.set_ylabel('Wind Speed')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_smard_openmeteo(df_smard.tail(30*24), df_om.tail(30*24), df_om_f, om_suffix='_hsee')\n",
    "    "
   ],
   "id": "65029a895b6849cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_es.tail(24*30).plot(figsize=(16,5))",
   "id": "e2eaa2f9de299bc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7caa79a74550c91e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_es.tail()",
   "id": "3827f061486e62c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_time_series(df, df_f, suffix, figsize=(15, 20)):\n",
    "    \"\"\"\n",
    "    Visualizes each column with a specified suffix in separate panels with a shared X-axis.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame with time-series data indexed by pd.Timestamp.\n",
    "    - suffix (str): Suffix to filter the columns of interest.\n",
    "    - figsize (tuple): Size of the figure for the plots.\n",
    "    \"\"\"\n",
    "    # Filter columns based on the suffix\n",
    "    filtered_columns = [col for col in df.columns if col.endswith(suffix)]\n",
    "\n",
    "    if not filtered_columns:\n",
    "        raise ValueError(f\"No columns found with suffix '{suffix}' in the DataFrame.\")\n",
    "\n",
    "    num_columns = len(filtered_columns)\n",
    "\n",
    "    # Create a shared X-axis for all subplots\n",
    "    fig, axes = plt.subplots(num_columns, 1, figsize=figsize, sharex=True)\n",
    "    if num_columns == 1:\n",
    "        axes = [axes]  # Ensure axes is always iterable\n",
    "\n",
    "    for ax, column in zip(axes, filtered_columns):\n",
    "        ax.plot(df.index, df[column], label=column)\n",
    "        ax.plot(df_f.index, df_f[column], ls='--')\n",
    "        ax.set_ylabel(column.replace(suffix, '').replace('_', ' ').capitalize())\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Set shared X-axis label\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "visualize_time_series(df=df_om,df_f = df_om_f, suffix='hsee')"
   ],
   "id": "4096d2079bfd61c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "def",
   "id": "b676f6b909e6689c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "def plot_multivariate_timeseries(df, df_fore):\n",
    "    # Create a subplot with 3 rows\n",
    "    fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.02)\n",
    "\n",
    "    # Plot 'DA_auction_price' on the first subplot\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['DA_auction_price'], name='DA Auction Price'),\n",
    "                  row=1, col=1)\n",
    "\n",
    "    # Plot 'total_gen', 'total_grid_load', 'residual_load_forecast' on the second subplot\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['total_gen'], name='Total Generation'), row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['total_grid_load'], name='Total Grid Load'), row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['residual_load_forecast'], name='Residual Load Forecast'), row=2, col=1)\n",
    "\n",
    "    # Plot 'temperature_2m_hsee' and 'temperature_2m_solw' on the third subplot\n",
    "    for df_,ls in zip([df, df_fore],['solid', 'dash']):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_.index,\n",
    "            y=df_['temperature_2m_hsee'],\n",
    "            name='Temperature 2m HSEE',\n",
    "            line=dict(dash=ls,color='blue')\n",
    "        ), row=3, col=1)\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_.index,\n",
    "            y=df_['temperature_2m_solw'],\n",
    "            name='Temperature 2m SOLW',\n",
    "            line=dict(dash=ls,color='green')\n",
    "        ), row=3, col=1)\n",
    "    # Add a vertical line for today across all subplots\n",
    "    # today_line = dict(type='line', x0=pd.Timestamp.today(), y0=0, x1=pd.Timestamp.today(),\n",
    "    #                   y1=1, xref='x', yref='paper', line=dict(color='black', width=2, dash='dash'))\n",
    "    # fig.add_shape(today_line, row='all', col=1)\n",
    "    fig.add_vline(x=pd.Timestamp.today(), line_width=1, line_dash=\"dash\", line_color=\"gray\")\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(height=900, title='Multivariate Time Series Analysis',\n",
    "                      xaxis_title='DateTime', yaxis_title='Values')\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "plot_multivariate_timeseries(df=df_hist.tail(1000), df_fore = df_fore)"
   ],
   "id": "57ad1223135ac35b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plot SMARD data",
   "id": "11c82d1e0362cc38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_modules.collect_data_smard import DataEnergySMARD\n",
    "countries = list(DataEnergySMARD.country_map.keys())\n",
    "# Create a subplot with 3 rows\n",
    "fig = make_subplots(rows=len(countries), cols=1, shared_xaxes=True, vertical_spacing=0.02)\n",
    "\n",
    "df_hist_ = df_hist.tail(4*7*24)\n",
    "for i, country in enumerate(countries):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_hist_.index,\n",
    "        y=df_hist_[f'{country}_export'],\n",
    "        name=f'{country}',\n",
    "        line=dict(color='blue')  # Set the line color here\n",
    "    ), row=i+1, col=1)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_hist_.index,\n",
    "        y=df_hist_[f'{country}_import'],\n",
    "        name=None,#f'{country}',\n",
    "        line=dict(color='red')  # Set the line color here\n",
    "    ), row=i+1, col=1)\n",
    "# Update layout\n",
    "fig.update_layout(height=1800, title='International trade',\n",
    "xaxis_title='DateTime', yaxis_title='Values')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ],
   "id": "c3c3dcbfafabbfd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6e5cac98f5e74214",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
